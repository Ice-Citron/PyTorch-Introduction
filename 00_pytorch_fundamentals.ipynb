{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BMPEtyL1aZby",
        "g9bHTR3jkuDr",
        "VGLJ7mHWr9_2",
        "BhayhfGI03J5",
        "9x-m2P_X7pU9",
        "QROJdy7bD0T-",
        "iPHKgZUXK1KA",
        "FT3gAnDZNi7M",
        "eIwaJfMb9YyP",
        "aSCq3ZqZBBsS",
        "7uayyyXqEmXs",
        "fzU4upg3F3d6",
        "ZwKX1QEe-MfB",
        "1tCDUypED0sO",
        "qF_SMASmkuf-",
        "W3wSP3fotY_i",
        "S3RRFTyGuJ0x",
        "Oqi8yguf1GPW",
        "eTWxoO1k0yFp",
        "Y_VYrwjwt_S2",
        "ZQD8bpqOChsw",
        "5dWcE3rWMski",
        "O0K3zfBbwa16",
        "SSpMm-0nwnJK",
        "ZrINWfJI3r5h",
        "h0uwxrELYKf_",
        "9lZ2df4NY7Y6",
        "7_DatbmFS-6q",
        "hJXgjgeV-0sh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Resource Link: https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises"
      ],
      "metadata": {
        "id": "fPCWsihGzXzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOSmuwB2LEhj",
        "outputId": "926a8672-4acd-42b4-c5d5-6e23e524f281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESnUlcanGGd3"
      },
      "outputs": [],
      "source": [
        "#!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Tensors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FCG5INYwysxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Tensors\n"
      ],
      "metadata": {
        "id": "BMPEtyL1aZby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "PyTorch tensors are created using torch.Tensor() = https://pytorch.org/docs/stable/tensors.html\n",
        "\n",
        "---\n",
        "\n",
        "* Scalar (Rank 0 Tensor): A scalar is a single number. In PyTorch, you create a scalar by passing a single value to torch.tensor(). The code creates a scalar with a value of 7. .ndim checks the number of dimensions (which is 0 for a scalar), .shape returns an empty tuple since scalars have no dimensions, and .item() converts the tensor scalar to a standard Python scalar.\n",
        "\n",
        "* Vector (Rank 1 Tensor): A vector is a 1-dimensional array of numbers. The code creates a vector [7, 7] using torch.tensor(). Here, .ndim returns 1 indicating it's 1-dimensional, and .shape returns (2), meaning the vector has 2 elements.\n",
        "\n",
        "* Matrix (Rank 2 Tensor): A matrix is a 2-dimensional array of numbers. The example creates a 2x2 matrix with torch.tensor([[7, 8], [9, 10]]). .ndim shows it's 2-dimensional, and .shape returns (2, 2), indicating 2 rows and 2 columns.\n",
        "\n",
        "* Higher-Dimensional Tensor (Rank 3+ Tensor): Tensors can have three or more dimensions. The provided code creates a tensor with dimensions (1, 3, 3), meaning it contains a 3x3 matrix in a 3-dimensional space. .ndim confirms it's 3-dimensional. The shape is described as having 1 matrix that contains 3 rows and 3 columns each.\n",
        "\n",
        "This code is a basic introduction to the concept of tensors in PyTorch, demonstrating how to create them and inspect their properties like dimensionality and shape. Understanding these properties is essential for working with data in machine learning and deep learning applications.\n",
        "\n",
        "---\n",
        "\n",
        "**Naming Conventions**\n",
        "* Scalar (dimension = 0) - lower-case $a$\n",
        "* Vector (dimension = 1) - lower-case $y$\n",
        "* Matrix (dimension = 2) - upper-case $Q$\n",
        "* Tensor (dimension ≥ 3) - upper-case $X$"
      ],
      "metadata": {
        "id": "zqifVxmRaBgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scalar (tensor rank 0)\n",
        "scalar = torch.tensor(7) # A tensor can be constructed from a Python list or sequence using the torch.tensor() constructor\n",
        "\n",
        "print(scalar)\n",
        "print(scalar.ndim) # checks how many dimensions does scalar have\n",
        "print(scalar.shape)\n",
        "print(scalar.item()) # Get tensor back as Python int\n"
      ],
      "metadata": {
        "id": "gD0W48aJy93i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d38ea7-150d-42d3-e8d0-e1f92aa5358a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7)\n",
            "0\n",
            "torch.Size([])\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The .item() method is specific to tensors that have a single element. Its purpose is to extract the value of that single element as a standard Python number (int or float). It's particularly useful when you're working with tensors that result from some computations, and you want to convert the result into a Python scalar.\n",
        "\n",
        "> Attempting to use .item() on a tensor with more than one element will result in an error because it's not clear which element should be converted to a Python scalar. The method is designed to ensure that it's only used when there's exactly one element in the tensor, thereby avoiding ambiguous situations."
      ],
      "metadata": {
        "id": "0TQh-ehvd9dD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector (tensor rank 1)\n",
        "vector = torch.tensor([7, 7])\n",
        "print(vector)\n",
        "print(vector.ndim) # checks how many dimension does vector have\n",
        "print(vector.shape) # outputs the shape of the vector, eg 2 by 1."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8dtOEDVW_nd",
        "outputId": "a62e544d-8510-4b55-e9e6-3c81335b7037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([7, 7])\n",
            "1\n",
            "torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix (tensor rank 2)\n",
        "MATRIX = torch.tensor([[7, 8], [9, 10]])\n",
        "print(MATRIX)\n",
        "print(MATRIX.ndim) # checks how many dimension does MATRIX have\n",
        "print(MATRIX.shape) # outputs the shape of the matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT1E57eiYTLz",
        "outputId": "6d654339-b8fd-405b-83ef-09655b9e62e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 7,  8],\n",
            "        [ 9, 10]])\n",
            "2\n",
            "torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor (tensor rank 3+)\n",
        "TENSOR = torch.tensor([[[1, 2, 3],   #[ <-- dim=0[ <-- dim=1 [ <-- dim=2 ]]]\n",
        "                        [4, 5, 6],\n",
        "                        [6, 7, 8]]])\n",
        "print(TENSOR)\n",
        "print(TENSOR.ndim) # checks how many dimension does TENSOR have\n",
        "\n",
        "print(TENSOR.shape) # outputs the shape of the tensor\n",
        "                    # torch.Size([1, 3, 3]) means that you have 1x 3 by 3 matrix in this tensor.\n",
        "                    # torch.Size([dim=0, dim=1, dim=2])\n",
        "                    # alternatively, dim=0 contains 1 value (matrix), dim=1 contains 3 values (vector), dim=2 contains 3 scalars each.\n"
      ],
      "metadata": {
        "id": "w8NT_D3hW0NI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f0c59a-1a73-4911-ec1d-934681879eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1, 2, 3],\n",
            "         [4, 5, 6],\n",
            "         [6, 7, 8]]])\n",
            "3\n",
            "torch.Size([1, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Tensors\n",
        "\n"
      ],
      "metadata": {
        "id": "g9bHTR3jkuDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why random tensors?**\n",
        "\n",
        "Random tensors are important because the way many neural network learn is that they start with tensors full of random numbers and then adjust those random numbers to better represent the data.\n",
        "\n",
        "Workflow:\n",
        "\n",
        "> `Start with random numbers -> Look at data -> Update random numbers -> Look at data -> Update random numbers`\n",
        "\n",
        "PyTorch Docs (torch.rand()) url - https://pytorch.org/docs/stable/generated/torch.rand.html\n"
      ],
      "metadata": {
        "id": "SXvKoBOIaPKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random tensor of size (4, 3, 4)\n",
        "random_tensor = torch.rand(4, 3, 4) # can write \"touch.rand(size=(4, 3, 4))\" // both are acceptable\n",
        "print(random_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3OAoLvKldT6",
        "outputId": "b4787094-11bb-4763-b44e-686484873e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.2227, 0.3177, 0.4999, 0.3556],\n",
            "         [0.8992, 0.9434, 0.0417, 0.9339],\n",
            "         [0.3301, 0.1877, 0.0852, 0.7816]],\n",
            "\n",
            "        [[0.6055, 0.9440, 0.7531, 0.6954],\n",
            "         [0.5516, 0.0211, 0.6532, 0.5071],\n",
            "         [0.6850, 0.4792, 0.9160, 0.8725]],\n",
            "\n",
            "        [[0.6563, 0.2124, 0.5223, 0.3818],\n",
            "         [0.7180, 0.8954, 0.1314, 0.2712],\n",
            "         [0.4022, 0.8065, 0.3183, 0.6993]],\n",
            "\n",
            "        [[0.2008, 0.2597, 0.3368, 0.9321],\n",
            "         [0.2766, 0.8006, 0.6173, 0.6912],\n",
            "         [0.1076, 0.0813, 0.6475, 0.2482]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_tensor.ndim) # checks how many dimension does random_tensor have\n",
        "print(random_tensor.shape) # outputs the shape of the random_tensor"
      ],
      "metadata": {
        "id": "pyNw82U6rz-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56659222-9614-450f-d30a-c9bac9030529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "torch.Size([4, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random tensor with simillar shape to an image tensor\n",
        "random_image_size_tensor = torch.rand(size=(224, 224, 3)) # alternative is colour channel coming first, eg. size=(3, 224, 224) // both image dimension of 224x224\n",
        "print(random_image_size_tensor.ndim) # checks dimension of random_image_size_tensor\n",
        "print(random_image_size_tensor.shape) # outputs shape of random_image_size_tensor\n",
        "\n",
        "# random tensors are so valuable because neural network starts with random data -> look at data -> update the random numbers -> look at data ....."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IptIzup9r1zM",
        "outputId": "e2d3945c-78ef-42a8-881f-c791710c7b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "torch.Size([224, 224, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zeros and Ones"
      ],
      "metadata": {
        "id": "VGLJ7mHWr9_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor of all zeroes\n",
        "zeros = torch.zeros(size=(3, 4))\n",
        "print(zeros)\n",
        "print(zeros * random_tensor) # multiply zero tensor with random tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dTV48tbyNit",
        "outputId": "d90c9386-dc8c-472c-e31d-1cb2942ed1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor of all ones\n",
        "ones = torch.ones(size=(3, 4))\n",
        "print(ones)\n",
        "print(ones * random_tensor) # multiply zero tensor with random tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65035d5c-89ab-4f9a-c478-c834da52c044",
        "id": "65SLMGmxywbK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "tensor([[[0.2227, 0.3177, 0.4999, 0.3556],\n",
            "         [0.8992, 0.9434, 0.0417, 0.9339],\n",
            "         [0.3301, 0.1877, 0.0852, 0.7816]],\n",
            "\n",
            "        [[0.6055, 0.9440, 0.7531, 0.6954],\n",
            "         [0.5516, 0.0211, 0.6532, 0.5071],\n",
            "         [0.6850, 0.4792, 0.9160, 0.8725]],\n",
            "\n",
            "        [[0.6563, 0.2124, 0.5223, 0.3818],\n",
            "         [0.7180, 0.8954, 0.1314, 0.2712],\n",
            "         [0.4022, 0.8065, 0.3183, 0.6993]],\n",
            "\n",
            "        [[0.2008, 0.2597, 0.3368, 0.9321],\n",
            "         [0.2766, 0.8006, 0.6173, 0.6912],\n",
            "         [0.1076, 0.0813, 0.6475, 0.2482]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# datatype\n",
        "print(zeros.dtype)\n",
        "print(ones.dtype)\n",
        "print(random_tensor.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTjB927ny7Nc",
        "outputId": "d9cfd4c6-c0f3-4a11-ed13-94f5bfc805ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "torch.float32\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Range of Tensors and Tensors-like\n",
        "\n"
      ],
      "metadata": {
        "id": "BhayhfGI03J5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch docs for \"torch.arange()\" - https://pytorch.org/docs/stable/generated/torch.arange.html"
      ],
      "metadata": {
        "id": "jnZPjW9tbLID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.range(0, 10) # depracated method, use \"torch.arange()\" instead\n",
        "one_to_ten = torch.arange(0, 10) # tensor.arange() starts at start_int, ends at end_int - 1.\n",
        "print(one_to_ten)\n",
        "one_to_k = torch.arange(start=0, end=1000, step=77)\n",
        "print(one_to_k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K87hTXL-0-8v",
        "outputId": "40c3f28a-7df2-4adb-9875-65b044737833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "tensor([  0,  77, 154, 231, 308, 385, 462, 539, 616, 693, 770, 847, 924])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tensors like\n",
        "ten_zeros = torch.zeros_like(input=one_to_ten) # this will get you a tensor full of zeros, which is the same shape as the tensor input, ie \"one_to_ten\"\n",
        "print(ten_zeros)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSKlSEhj1IE6",
        "outputId": "858d0282-c916-4066-a281-e76ab2bed990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Datatypes\n",
        "\n"
      ],
      "metadata": {
        "id": "9x-m2P_X7pU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Tensor datatypes is one of the 3 big errors you'll run into with PyTorch & deep learning:\n",
        "1. Tensors not right datatype (eg. happens when inconsistent dtype substitution between tensors)\n",
        "2. Tensors not right shape (eg. happens in matrix multiplication)\n",
        "3. Tensors not on the right device (eg. device=\"cpu\" vs device=\"cuda\", and device agnostic code)"
      ],
      "metadata": {
        "id": "5hlfRLavbI2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Float 32 tensor\n",
        "float_32_tensor = torch.tensor([1.0, 2.0, 3.0], # even if dtype=None, PyTorch will then revert to default datatype\n",
        "                               dtype=None, # what datatype is the tensor (e.g float32 or float16)\n",
        "                               device=None, # what device is your tensor on\n",
        "                               requires_grad=False) # whether or not to track gradients with this tensor's operation\n",
        "print(float_32_tensor)\n",
        "print(float_32_tensor.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyiad9FQ7uEW",
        "outputId": "888b24be-2a69-4d4d-d7f2-6fef9623e6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "float_16_tensor = float_32_tensor.to(dtype=torch.float16) # convert float_32_tensor to float_16_tensor // alternatively, torch.half would also work\n",
        "print(float_16_tensor)\n",
        "print(float_16_tensor.dtype) # float_16_tensor is now float 16"
      ],
      "metadata": {
        "id": "Ac4tbfNB_mKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa273f14-8520-4891-aae9-a36c4cbfdcdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.], dtype=torch.float16)\n",
            "torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# suprisingly no error raised even though discrepancy in dtype; normal behavious, but can be an issue with large neural network,\n",
        "# but preferably don't use different' dtype\n",
        "print(float_16_tensor * float_32_tensor) # works even if multipled by torch.long or torch.int32, etc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYBr0XYkANJq",
        "outputId": "f68a2571-a062-451a-f622-a250e634a399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 4., 9.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Information from Tensors (Tensor Attributes)\n",
        "\n"
      ],
      "metadata": {
        "id": "QROJdy7bD0T-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tensors not right datatype - to do get datatype from a tensor, can use `tensor.dtype`\n",
        "2. Tensors not right shape - to get a shape from a tensor, can use `tensor.shape`\n",
        "3. Tensors not on the right device - to get device from a tensor, can use `tensor.device`"
      ],
      "metadata": {
        "id": "5VnseeYobGF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a test tensor\n",
        "some_tensor = torch.rand(2, 3, 4)\n",
        "print(some_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rlMn3R_GFVI",
        "outputId": "740b66b1-fdb2-4733-cf4e-20ac602188a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.2966, 0.0125, 0.2912, 0.5563],\n",
            "         [0.0746, 0.3665, 0.7442, 0.1715],\n",
            "         [0.1300, 0.2580, 0.5232, 0.0725]],\n",
            "\n",
            "        [[0.6415, 0.7326, 0.1114, 0.6876],\n",
            "         [0.4124, 0.7489, 0.5754, 0.8741],\n",
            "         [0.4082, 0.9335, 0.6183, 0.2421]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find out details about some_tensor\n",
        "print(f\"Datatype of tensor : {some_tensor.dtype }\")\n",
        "print(f\"Shape of tensor    : {some_tensor.shape }\")  # \"some_tensor.shape\" and \"some_tensor.size()\" does the same thing (shape is attribute, .size() is founction)\n",
        "print(f\"Device of tensor   : {some_tensor.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ8mtac0GGAq",
        "outputId": "c04a84a1-a758-4560-cefa-28ab194e36b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datatype of tensor : torch.float32\n",
            "Shape of tensor    : torch.Size([2, 3, 4])\n",
            "Device of tensor   : cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manipulating Tensors (Tensor Operations)\n",
        "\n"
      ],
      "metadata": {
        "id": "iPHKgZUXK1KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor Operations include:\n",
        "* Addition\n",
        "* Subtraction\n",
        "* Multiplication (element-wise)\n",
        "* Division\n",
        "* Matrix Multiplication"
      ],
      "metadata": {
        "id": "OhTfE7inaiHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Tensor\n",
        "tensor = torch.tensor([1, 2, 3]) # initialised from Python list or array\n",
        "print(tensor)\n",
        "print(tensor + 10) # addition\n",
        "print(tensor - 10) # subtraction\n",
        "print(tensor * 10) # multiplication\n",
        "print(tensor / 10) # division"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdF00hiMLhZ-",
        "outputId": "856d368c-e2c5-4b75-9b95-b62fd4d613ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([11, 12, 13])\n",
            "tensor([-9, -8, -7])\n",
            "tensor([10, 20, 30])\n",
            "tensor([0.1000, 0.2000, 0.3000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try out PyTorch built-in functions\n",
        "print(torch.add(tensor, 10)) # addition\n",
        "print(torch.sub(tensor, 10)) # subtraction\n",
        "print(torch.mul(tensor, 10)) # multiplication\n",
        "print(torch.div(tensor, 10)) # division\n",
        "\n",
        "print(torch.abs(tensor)) # absolute value\n",
        "print(torch.sqrt(tensor)) # square root\n",
        "print(torch.exp(tensor)) # exponential"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSAXIojJLzyz",
        "outputId": "d48cd11a-d8ab-4470-d7d7-9b7f3a7a0614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([11, 12, 13])\n",
            "tensor([-9, -8, -7])\n",
            "tensor([10, 20, 30])\n",
            "tensor([0.1000, 0.2000, 0.3000])\n",
            "tensor([1, 2, 3])\n",
            "tensor([1.0000, 1.4142, 1.7321])\n",
            "tensor([ 2.7183,  7.3891, 20.0855])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix Multiplication\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FT3gAnDZNi7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two main ways of performing multiplication in neural networks and deep learning are:\n",
        "1. Element-wise multiplication\n",
        "2. Matrix multiplication (dot product) [dot product and matrix multiplication is used interchangeably]\n",
        "\n",
        "URL: https://www.mathsisfun.com/algebra/matrix-multiplying.html\n",
        "\n",
        "Random Note: To multiply a matrix by another matrix, we need to do the \"dot product\" of rows and columns... (rewatch 3Blue1Brown if recap needed on linear algebra)\n",
        "\n",
        "---\n",
        "\n",
        "Regarding anatomy of Matrix, below as an example is a **4x2 matrix**, pay attention that the int*int order of matrix is classified by `_rows_x_columns_`. Use `tensor.shape`, to find the dimensions of the matrix:\n",
        "\n",
        "`[`\n",
        "\n",
        ">  `[1, 2],`\n",
        "\n",
        ">  `[3, 4],`\n",
        "\n",
        ">  `[5, 6],`\n",
        "\n",
        ">  `[7, 8]`\n",
        "\n",
        "`]`\n",
        "\n",
        "There are 2 main rules that performing matrix multiplication needs to satisfy:\n",
        "\n",
        "1. The **inner dimensions** must match:\n",
        "\n",
        "* `(3x2) @ (3x2)` <-- this won't work\n",
        "* `(2x3) @ (3x2)` <-- this will work\n",
        "* `(3x2) @ (2x2)` <-- this will work\n",
        "\n",
        "2. The resulting matrix has the shape of the **outer dimension**:\n",
        "\n",
        "* `(4x3) @ (3x6) = (4x6)` <-- 4x6 matrix outputted\n",
        "* `(3x1) @ (1x7) = (3x7)` <-- 3x7 matrix outputted\n",
        "\n",
        "\n",
        "> Note: regarding matrix multiplication syntax, `tensor @ tensor`, is the same as `torch.matmul(tensor, tensor)`, same performance wise etc, the '@' operator is an overload of the \\__matmul__ magic method in the PyTorch's tensor class."
      ],
      "metadata": {
        "id": "dPfobbYqafF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Element-wise Multiplication\n",
        "print(tensor, \"*\", tensor)\n",
        "print(f\"Equals: {tensor * tensor}\")\n",
        "\n",
        "print(torch.rand(4, 2).shape) # outputs the shape of a 4x2 matrix\n",
        "print((torch.rand(4, 3) @ torch.rand(3, 6)).shape) # 4x3 @ 3x6 -> 4x6\n",
        "print((torch.rand(3, 1) @ torch.rand(1, 7)).shape) # 3x1 @ 1x7 -> 3x7"
      ],
      "metadata": {
        "id": "g_F480MvNiqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe1e33e-f91b-4c3c-b84e-79d1e47a68c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3]) * tensor([1, 2, 3])\n",
            "Equals: tensor([1, 4, 9])\n",
            "torch.Size([4, 2])\n",
            "torch.Size([4, 6])\n",
            "torch.Size([3, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix Multiplication\n",
        "torch.matmul(tensor, tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpXkhMZ4PttW",
        "outputId": "e7de6406-dcd9-4a8b-f3c6-b49aaf0e5617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(14)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "value=0\n",
        "for i in range(len(tensor)):\n",
        "    value += tensor[i] * tensor[i]\n",
        "print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4Bf7C3xQCWX",
        "outputId": "fb98ff95-9b43-4730-9422-789084dc4245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(14)\n",
            "CPU times: user 2.09 ms, sys: 202 µs, total: 2.29 ms\n",
            "Wall time: 2.52 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "torch.matmul(tensor, tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwZo9YICQqyc",
        "outputId": "f6bb2358-afa0-4c20-b49b-4bd09de17171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 101 µs, sys: 19 µs, total: 120 µs\n",
            "Wall time: 127 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(14)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One of the Most Common Errors in Deep Learning: Shape Errors\n",
        "\n"
      ],
      "metadata": {
        "id": "eIwaJfMb9YyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes for Matrix Multiplication\n",
        "\n",
        "tensor_A = torch.tensor([[1, 2],\n",
        "                         [3, 4],\n",
        "                         [5, 6]])\n",
        "\n",
        "tensor_B = torch.tensor([[7, 10, 1],\n",
        "                         [8, 11, 2]])\n",
        "\n",
        "#torch.mm(tensor_A, tensor_B)  # torch.mm() is the same as torch.matmul() // It's an Alias.\n",
        "torch.matmul(tensor_A, tensor_B) # As see, need to be 3x2 @ 2x3, which is multipliable. Which this outputs 3x3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auJ06dh8-K_S",
        "outputId": "600bc212-d5b2-40c8-b751-4ea8332675af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 23,  32,   5],\n",
              "        [ 53,  74,  11],\n",
              "        [ 83, 116,  17]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A way to fix our tensor shape issues, is that we can manipulate the shape of one of our tensors using a **transpose**.\n",
        "\n",
        "A **transpose** switches the axes or dimensions of a given tensor."
      ],
      "metadata": {
        "id": "lsVwPHOl_F8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Shape:\", tensor_A.shape)\n",
        "print(\"Transposed Shape:\", tensor_A.T.shape)"
      ],
      "metadata": {
        "id": "oih1wrZE-GuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92cad43-fa6a-4934-94e9-07d0ebc2d58f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Shape: torch.Size([3, 2])\n",
            "Transposed Shape: torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the min, max, mean, sum, etc. (Tensor Aggregation)\n",
        "\n"
      ],
      "metadata": {
        "id": "aSCq3ZqZBBsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of \"Aggregation\": The formation of a number of things into a cluster.\n"
      ],
      "metadata": {
        "id": "9GD6BHXqbNQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Tensor\n",
        "\n",
        "x = torch.arange(0, 100, 10)\n",
        "print(x)\n",
        "\n",
        "print(torch.min(x), x.min()) # Find the Min\n",
        "print(torch.max(x), x.max()) # Find the Max\n",
        "print(torch.mean(x.float()), x.type(torch.float32).mean()) # Find the Mean // When calculating mean, needs to be in float or complex dtype. Long doesn't work. Refer to MISC.\n",
        "print(torch.sum(x), x.sum()) # Find the Sum\n"
      ],
      "metadata": {
        "id": "uBSiMvbfBHv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e99227-ca23-4a89-fdc0-659afad28a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
            "tensor(0) tensor(0)\n",
            "tensor(90) tensor(90)\n",
            "tensor(45.) tensor(45.)\n",
            "tensor(450) tensor(450)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the Positional min and max"
      ],
      "metadata": {
        "id": "7uayyyXqEmXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x) # outputs \"tensor.arange(0, 100, 10)\"\n",
        "\n",
        "# Find the position in tensor that has the minimum value with \"argmin()\" -> returns index position of target tensor where the minimum value occurs\n",
        "print(x.argmin(), x[x.argmin()])\n",
        "\n",
        "# Find the position in tensor that has the maximum value with \"argmax()\" -> returns index position of target tensor where the maximum value occurs\n",
        "print(x.argmax(), x[x.argmax()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bfpLR4jEp3C",
        "outputId": "7addb446-c5bb-4464-f884-4573b70d3a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
            "tensor(0) tensor(0)\n",
            "tensor(9) tensor(90)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshaping, Stacking, Squeezing and Unsqueezing Tensors\n",
        "\n"
      ],
      "metadata": {
        "id": "fzU4upg3F3d6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Reshaping - Reshapes an input tensor to a defined shape, eg 1x12 matrix to a 2x6 matrix of a 4x3 matrix\n",
        "* View - Return a view of an input tensor of certain shape but keep the same memory as the original tensor\n",
        "* Stacking - combine multiple tensors on top of each other (vstack) or side by side (hstack)\n",
        "* Squeeze - removes all '1' dimensions from a tensor\n",
        "* Unsqueeze - add a '1' dimension to a target tensor\n",
        "* Permute - Return a view of the input with dimensions permuted (swapped) in a certain way"
      ],
      "metadata": {
        "id": "UpEBu9Diaqco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(0., 12.)\n",
        "print(x, x.shape, '\\n')\n",
        "\n",
        "# Add an Extra Dimension -- tensor.reshape()\n",
        "x_reshaped = x.reshape(2, 6)  # reshapes input 1x12 matrix to 2x6 matrix\n",
        "print(\"Reshapes 1x12 matrix to 2x6 matrix\")\n",
        "print(x_reshaped, x_reshaped.shape, '\\n')\n",
        "\n",
        "x_reshaped = x.reshape(4, 3)  # reshapes input 1x12 matrix to 4x3 matrix\n",
        "print(\"Reshapes 1x12 matrix to 4x3 matrix\")\n",
        "print(x_reshaped, x_reshaped.shape, '\\n')\n",
        "\n",
        "# Quick note regarding tensor.shape\n",
        "#   For Scalar, it will results in \"torch.Size([])\"                    <-- indicating 0th dimension\n",
        "#   For Vector, it will results in \"torch.Size([int])\"                 <-- indicating 1st dimension\n",
        "#   For Matrix, it will results in \"torch.Size([int, int])\"            <-- indicating 2nd dimension\n",
        "#   For Tensor, it will results in \"torch.Size([int, int, int, ...])\"  <-- indicating 3+  dimension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR12jVwrGiNe",
        "outputId": "67123fbc-514d-4b8d-d2aa-d52d35b3d38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]) torch.Size([12]) \n",
            "\n",
            "Reshapes 1x12 matrix to 2x6 matrix\n",
            "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
            "        [ 6.,  7.,  8.,  9., 10., 11.]]) torch.Size([2, 6]) \n",
            "\n",
            "Reshapes 1x12 matrix to 4x3 matrix\n",
            "tensor([[ 0.,  1.,  2.],\n",
            "        [ 3.,  4.,  5.],\n",
            "        [ 6.,  7.,  8.],\n",
            "        [ 9., 10., 11.]]) torch.Size([4, 3]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newTensor = torch.arange(0., 12.)\n",
        "\n",
        "# Change the View -- tensor.view()\n",
        "tempView = newTensor.view(2, 6)\n",
        "print(tempView, tempView.shape)\n",
        "\n",
        "#Changing \"tempView\" tensor variable, changes \"x\" tensor variable (because a view of a tensor shares the same memory as the original input)\n",
        "tempView[:, 3] = 100  # refer to MISC regarding slicing\n",
        "print(\"\\n\", tempView)\n",
        "print(newTensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dso3LO4aIBCJ",
        "outputId": "88b80bf4-424c-48c8-e440-e10f65aa8e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
            "        [ 6.,  7.,  8.,  9., 10., 11.]]) torch.Size([2, 6])\n",
            "\n",
            " tensor([[  0.,   1.,   2., 100.,   4.,   5.],\n",
            "        [  6.,   7.,   8., 100.,  10.,  11.]])\n",
            "tensor([  0.,   1.,   2., 100.,   4.,   5.,   6.,   7.,   8., 100.,  10.,  11.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack tensors on top of each other\n",
        "x_stacked = torch.stack((x, x, x, x), dim=0)\n",
        "print(x_stacked, x_stacked.shape)\n",
        "x_stacked = torch.stack((x, x, x, x), dim=1)\n",
        "print(x_stacked, x_stacked.shape)\n",
        "\n",
        "x_stacked = torch.stack((x, x, x, x), dim=-1) # dim=-1 refers to last dimension of tensor\n",
        "print(x_stacked, x_stacked.shape)\n",
        "x_stacked = torch.stack((x, x, x, x), dim=-2) # dim=-2 refers to second to last dimension of tensor\n",
        "print(x_stacked, x_stacked.shape)\n",
        "\n",
        "# NOTE: Refer to MISC on Stack in PyTorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZl-Z-asuc3R",
        "outputId": "824cbc0e-9a57-46a0-db72-378e5f913e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
            "        [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
            "        [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
            "        [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]]) torch.Size([4, 12])\n",
            "tensor([[ 0.,  0.,  0.,  0.],\n",
            "        [ 1.,  1.,  1.,  1.],\n",
            "        [ 2.,  2.,  2.,  2.],\n",
            "        [ 3.,  3.,  3.,  3.],\n",
            "        [ 4.,  4.,  4.,  4.],\n",
            "        [ 5.,  5.,  5.,  5.],\n",
            "        [ 6.,  6.,  6.,  6.],\n",
            "        [ 7.,  7.,  7.,  7.],\n",
            "        [ 8.,  8.,  8.,  8.],\n",
            "        [ 9.,  9.,  9.,  9.],\n",
            "        [10., 10., 10., 10.],\n",
            "        [11., 11., 11., 11.]]) torch.Size([12, 4])\n",
            "tensor([[ 0.,  0.,  0.,  0.],\n",
            "        [ 1.,  1.,  1.,  1.],\n",
            "        [ 2.,  2.,  2.,  2.],\n",
            "        [ 3.,  3.,  3.,  3.],\n",
            "        [ 4.,  4.,  4.,  4.],\n",
            "        [ 5.,  5.,  5.,  5.],\n",
            "        [ 6.,  6.,  6.,  6.],\n",
            "        [ 7.,  7.,  7.,  7.],\n",
            "        [ 8.,  8.,  8.,  8.],\n",
            "        [ 9.,  9.,  9.,  9.],\n",
            "        [10., 10., 10., 10.],\n",
            "        [11., 11., 11., 11.]]) torch.Size([12, 4])\n",
            "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
            "        [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
            "        [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
            "        [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]]) torch.Size([4, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_reshaped)\n",
        "print(x_reshaped.shape, '\\n')\n",
        "\n",
        "# torch.unsqueeze() - adds a single dimension to a target tensor at a specific dim (dimension)\n",
        "x_unsqueezed = x_reshaped.unsqueeze(dim=0).unsqueeze(dim=2) # turns torch.Size([4, 3]) to torch.Size([1, 4, 1, 3])    // adding 2 dimensions in this case\n",
        "\n",
        "print(x_unsqueezed)\n",
        "print(x_unsqueezed.shape, '\\n')\n",
        "\n",
        "# torch.squeeze() - removes all single dimensions from a target tensor\n",
        "x_squeezed = x_reshaped.squeeze() # turns torch.Size([1, 4, 1, 3]) to torch.Size([4, 3]) // All single-dimensions removed\n",
        "\n",
        "print(x_squeezed)\n",
        "print(x_squeezed.shape, '\\n')"
      ],
      "metadata": {
        "id": "h6qmKxT1ucli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186a8844-db02-491a-a2ab-02c508b88cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.],\n",
            "        [ 3.,  4.,  5.],\n",
            "        [ 6.,  7.,  8.],\n",
            "        [ 9., 10., 11.]])\n",
            "torch.Size([4, 3]) \n",
            "\n",
            "tensor([[[[ 0.,  1.,  2.]],\n",
            "\n",
            "         [[ 3.,  4.,  5.]],\n",
            "\n",
            "         [[ 6.,  7.,  8.]],\n",
            "\n",
            "         [[ 9., 10., 11.]]]])\n",
            "torch.Size([1, 4, 1, 3]) \n",
            "\n",
            "tensor([[ 0.,  1.,  2.],\n",
            "        [ 3.,  4.,  5.],\n",
            "        [ 6.,  7.,  8.],\n",
            "        [ 9., 10., 11.]])\n",
            "torch.Size([4, 3]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.permute() - rearranges the dimensions of a target tensor in a specified order\n",
        "colour = torch.rand(size=(4, 6, 3))\n",
        "print(colour)                         # [height, width, colour_channels]\n",
        "print(colour.shape, '\\n')\n",
        "\n",
        "# Permute the original colour tensor to rearrange the axis (or dim) order\n",
        "colour_permuted = colour.permute(2, 0, 1) # turns torch.Size([4, 3]) to torch.Size([3, 4])\n",
        "\n",
        "print(colour_permuted)\n",
        "print(colour_permuted.shape, '\\n')    # [colour_channels, height, width]\n",
        "\n",
        "# Indexing -- Original + Permuted\n",
        "print(colour[1, 5, 2], colour_permuted[2, 1, 5]) # both this line and below accesses the same value, notice how their ith index is changed based on .permute() ordering\n",
        "print(colour[2, 1, 2], colour_permuted[2, 2, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNZVFdq23e7P",
        "outputId": "bb4051b8-2252-4d63-90b2-a344b5e6349f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.5645, 0.1165, 0.1019],\n",
            "         [0.6365, 0.7198, 0.4303],\n",
            "         [0.3915, 0.4394, 0.0663],\n",
            "         [0.5587, 0.2058, 0.1887],\n",
            "         [0.1834, 0.5979, 0.0802],\n",
            "         [0.1227, 0.6293, 0.7242]],\n",
            "\n",
            "        [[0.0393, 0.2908, 0.0398],\n",
            "         [0.3694, 0.6484, 0.2823],\n",
            "         [0.5443, 0.5146, 0.0168],\n",
            "         [0.4561, 0.8653, 0.1424],\n",
            "         [0.1205, 0.0313, 0.7364],\n",
            "         [0.7920, 0.0541, 0.1390]],\n",
            "\n",
            "        [[0.5999, 0.4486, 0.7372],\n",
            "         [0.4481, 0.8352, 0.8899],\n",
            "         [0.9212, 0.1219, 0.3458],\n",
            "         [0.2967, 0.7259, 0.9953],\n",
            "         [0.3462, 0.4986, 0.4077],\n",
            "         [0.8274, 0.8302, 0.4886]],\n",
            "\n",
            "        [[0.8864, 0.9980, 0.4603],\n",
            "         [0.9205, 0.8473, 0.3068],\n",
            "         [0.0061, 0.0761, 0.8057],\n",
            "         [0.9956, 0.2334, 0.2998],\n",
            "         [0.0985, 0.1982, 0.5070],\n",
            "         [0.9418, 0.0557, 0.1459]]])\n",
            "torch.Size([4, 6, 3]) \n",
            "\n",
            "tensor([[[0.5645, 0.6365, 0.3915, 0.5587, 0.1834, 0.1227],\n",
            "         [0.0393, 0.3694, 0.5443, 0.4561, 0.1205, 0.7920],\n",
            "         [0.5999, 0.4481, 0.9212, 0.2967, 0.3462, 0.8274],\n",
            "         [0.8864, 0.9205, 0.0061, 0.9956, 0.0985, 0.9418]],\n",
            "\n",
            "        [[0.1165, 0.7198, 0.4394, 0.2058, 0.5979, 0.6293],\n",
            "         [0.2908, 0.6484, 0.5146, 0.8653, 0.0313, 0.0541],\n",
            "         [0.4486, 0.8352, 0.1219, 0.7259, 0.4986, 0.8302],\n",
            "         [0.9980, 0.8473, 0.0761, 0.2334, 0.1982, 0.0557]],\n",
            "\n",
            "        [[0.1019, 0.4303, 0.0663, 0.1887, 0.0802, 0.7242],\n",
            "         [0.0398, 0.2823, 0.0168, 0.1424, 0.7364, 0.1390],\n",
            "         [0.7372, 0.8899, 0.3458, 0.9953, 0.4077, 0.4886],\n",
            "         [0.4603, 0.3068, 0.8057, 0.2998, 0.5070, 0.1459]]])\n",
            "torch.Size([3, 4, 6]) \n",
            "\n",
            "tensor(0.1390) tensor(0.1390)\n",
            "tensor(0.8899) tensor(0.8899)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indexing (Selecting Data from Tensors)"
      ],
      "metadata": {
        "id": "ZwKX1QEe-MfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing with PyTorch is simillar to indexing with NumPy."
      ],
      "metadata": {
        "id": "YbPqUbiO-P_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Tensor\n",
        "x = torch.arange(0., 12.)\n",
        "print(x, '\\n')\n",
        "x = x.reshape(1, 3, 4)\n",
        "print(x, x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj3QUv5e-TKI",
        "outputId": "222d636f-ef1f-4752-fb04-a9af8ff0884a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]) \n",
            "\n",
            "tensor([[[ 0.,  1.,  2.,  3.],\n",
            "         [ 4.,  5.,  6.,  7.],\n",
            "         [ 8.,  9., 10., 11.]]]) torch.Size([1, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's index on our new tensor\n",
        "print(x[0])               # accesses dim=0\n",
        "print(x[0][1])            # accesses dim=1\n",
        "print(x[0][1][2], '\\n')   # accesses dim=2\n",
        "\n",
        "print(x[0])\n",
        "print(x[0, 1])\n",
        "print(x[0, 1, 2]) # [int, int, int] indexing is the same as [int][int][int]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKK4BIoD_unF",
        "outputId": "b7d13b41-b546-4d9c-b1b1-2856fe518470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "tensor([4., 5., 6., 7.])\n",
            "tensor(6.) \n",
            "\n",
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "tensor([4., 5., 6., 7.])\n",
            "tensor(6.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also use ':' (slicing) to select \"all\" of a target dimension\n",
        "print(x[0, :, 2], '\\n') # outputs the 3rd value from all values (arrays) of dim=1\n",
        "print(x[0, :, :], '\\n') # outputs all values from all values (arrays) in dim=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK5sMOMqAoAE",
        "outputId": "461cf065-d0ec-437e-8d86-8e64e552e5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  6., 10.]) \n",
            "\n",
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch Tensors && NumPy\n"
      ],
      "metadata": {
        "id": "1tCDUypED0sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NumPy is a very popular scientific Python numerical computing library.\n",
        "\n",
        "And because of this, PyTorch has functionality to interact with it.\n",
        "\n",
        "* Convert data in NumPy, to PyTorch Tensor --> `torch.from_numpy(ndarray)`\n",
        "* Convert PyTorch Tensor to NumPy data --> `torch.Tensor.numpy()`"
      ],
      "metadata": {
        "id": "hLZxDAtKAubW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy array to tensor\n",
        "array = np.arange(0.0, 8.0)\n",
        "tensor = torch.from_numpy(array) # Warning: when converting from NumPy -> PyTorch, PyTorch reflects NumPy's default dtype of float64, unless specified otherwise\n",
        "print(tensor)                   # float64 - NumPy default\n",
        "print(tensor.float())           # converted to float32\n",
        "print(tensor.double(), '\\n')    # converted to float64 (double)\n",
        "\n",
        "# Tensor to NumPy array\n",
        "array = tensor.numpy()\n",
        "print(array, array.dtype, '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBZduZaQAtsd",
        "outputId": "b1ac1646-077f-42dc-d98a-40135c7f750a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64)\n",
            "tensor([0., 1., 2., 3., 4., 5., 6., 7.])\n",
            "tensor([0., 1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64) \n",
            "\n",
            "[0. 1. 2. 3. 4. 5. 6. 7.] float64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the value of NumPy array, using \"array = array + 1\"\n",
        "array = tensor.numpy()\n",
        "array = array + 1                                                               # NOTE: Refer to notes below regarding behaviour\n",
        "print(\"array = array + 1: \", tensor, array, '\\n')\n",
        "\n",
        "# Changing the value of NumPy array, using \"array += 1\"\n",
        "tensor = tensor.float()\n",
        "array = tensor.numpy()\n",
        "array += 1                                                                      # NOTE: Refer to notes below regarding behaviour\n",
        "print(\"array += 1: \", tensor, array, '\\n')                                      # NOTICE HOW BOTH ARRAY AND TENSOR IS CHANGED HERE\n",
        "\n",
        "print(\"\\ndtype check for both Numpy array and PyTorch tensor\\n------\")\n",
        "print(array, array.dtype) # dtype of NumPy array is float32, because the dtype of tensor was float32 prior.\n",
        "print(tensor, tensor.dtype)"
      ],
      "metadata": {
        "id": "ekuhIvhgejXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53440be-2b30-4302-e0f6-bf139678a759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array = array + 1:  tensor([0., 1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64) [1. 2. 3. 4. 5. 6. 7. 8.] \n",
            "\n",
            "array += 1:  tensor([1., 2., 3., 4., 5., 6., 7., 8.]) [1. 2. 3. 4. 5. 6. 7. 8.] \n",
            "\n",
            "\n",
            "dtype check for both Numpy array and PyTorch tensor\n",
            "------\n",
            "[1. 2. 3. 4. 5. 6. 7. 8.] float32\n",
            "tensor([1., 2., 3., 4., 5., 6., 7., 8.]) torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you perform `array = array + 1` in NumPy, it creates a new array with the result of `array + 1` and then assigns this new array to the variable array. This operation does not modify the original array in place, but rather rebinds the name array to a new object. Therefore, the PyTorch tensor that was originally linked to the NumPy array will not reflect this change since the underlying data is not modified, just the binding.\n",
        "\n",
        "On the other hand, when you do `array += 1`, this is an in-place operation which modifies the existing array's data. Since the PyTorch tensor created by torch.from_numpy() shares the same memory as the NumPy array, the changes made in place to the NumPy array are also reflected in the PyTorch tensor.\n",
        "\n",
        "Here's a brief outline of what happens with each operation:\n",
        "\n",
        "* `array = array + 1`: Creates a new array with incremented values and assigns it to the variable array. The original array remains unmodified.\n",
        "* `array += 1`: Adds 1 to each element of the original array in place, which also updates the values in the linked PyTorch tensor.\n",
        "\n",
        "In the PyTorch documentation, this sharing of memory between NumPy arrays and PyTorch tensors is clearly stated, and it's important to be aware of this to avoid unexpected side effects when manipulating either of the objects."
      ],
      "metadata": {
        "id": "nLnnQicAii-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reproducibility (Random Seed)\n",
        "\n"
      ],
      "metadata": {
        "id": "qF_SMASmkuf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In Short How a Neural Network Learns:**\n",
        "\n",
        "---\n",
        "\n",
        "1. Start with random numbers\n",
        "2. Tensor Operations\n",
        "3. Update random numbers to try and make  them better representations of data\n",
        "4. Step 3 repeats, again, again, again.....\n",
        "\n",
        "---\n",
        "\n",
        "To reduce the randomness in neural networks and PyTorch, comes the concept of a **random seed**.\n",
        "\n",
        "Essentially, what the rndopm see does is \"flavour\" the randomness."
      ],
      "metadata": {
        "id": "9NPBdDLflPHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 2 random tensors\n",
        "random_tensor_A = torch.rand(3, 4)\n",
        "random_tensor_B = torch.rand(3, 4)\n",
        "\n",
        "print(random_tensor_A)\n",
        "print(random_tensor_B)\n",
        "\n",
        "print(random_tensor_A == random_tensor_B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyPZT9IGlQJZ",
        "outputId": "76c1c8d3-9d43-4718-abfb-01a0a7a852ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8853, 0.4271, 0.8289, 0.5907],\n",
            "        [0.6330, 0.7076, 0.4465, 0.5986],\n",
            "        [0.6570, 0.0455, 0.5694, 0.1602]])\n",
            "tensor([[0.5202, 0.7657, 0.4440, 0.3385],\n",
            "        [0.6085, 0.0650, 0.1643, 0.7576],\n",
            "        [0.1722, 0.3328, 0.0262, 0.7212]])\n",
            "tensor([[False, False, False, False],\n",
            "        [False, False, False, False],\n",
            "        [False, False, False, False]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make some random but reproducible tensors\n",
        "\n",
        "# Set the random seed\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random_tensor_C = torch.rand(3, 4)\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED) # Need to recall manual_speed(), if you want subsequent calls to have the same numbers in matrices.\n",
        "random_tensor_D = torch.rand(3, 4)\n",
        "\n",
        "print(random_tensor_C)\n",
        "print(random_tensor_D)\n",
        "\n",
        "print(random_tensor_C == random_tensor_D)"
      ],
      "metadata": {
        "id": "scKj2r7wov78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07bf1d5-ceac-4668-f800-2baae4e87f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
            "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
            "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
            "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
            "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
            "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
            "tensor([[True, True, True, True],\n",
            "        [True, True, True, True],\n",
            "        [True, True, True, True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resources\n",
        "* PyTorch docs on Reproducibility: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "* Wikipedia on Random Seed: https://www.wikiwand.com/en/Random_seed"
      ],
      "metadata": {
        "id": "t60FzaAfrzac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Tensors and PyTorch Objects on the GPUs (and making faster computations)\n"
      ],
      "metadata": {
        "id": "W3wSP3fotY_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPUs = faster computation on numbers, thanks to CUDA + Nvidia hardware + PyTorch working behind the scenes to make everything possible"
      ],
      "metadata": {
        "id": "-w2vIM0ytoTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Getting a GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "S3RRFTyGuJ0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Easiest - Using Google Colab - Cloud Computing (Free or Pro)\n",
        "2. Use personal GPU (investment required)\n",
        "3. Use Cloud Computing - GCP, AWS, Azure... these services allow you to rent computers on the cloud and access them.\n",
        "\n",
        "For Option 2 and Option 3, PyTorch + GPU drivers (CUDA) takes a little bit of setting up. To do this, refer to PyTorch setup documents:\n",
        "\n",
        ">https://pytorch.org/get-started/locally/"
      ],
      "metadata": {
        "id": "V-MewB_92XW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKoQFjDvwt2H",
        "outputId": "e9419307-5478-4cf4-c099-db0e43eb659b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Check for GPU access with PyTorch"
      ],
      "metadata": {
        "id": "Oqi8yguf1GPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For PyTorch, since it's capable of running compute on the GPU or CPU, it's best practice to setup device agnostic code:\n",
        "\n",
        ">https://pytorch.org/docs/stable/notes/cuda.html#best-practices\n",
        "\n",
        "I.e. run on GPU if available, else default to CPU"
      ],
      "metadata": {
        "id": "xPUHeYHQ1ybE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU access with PyTorch\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "# Count number of devices\n",
        "torch.cuda.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfmQYmbM1JXE",
        "outputId": "dcafd154-f1e5-4ed5-e2ab-67206c822575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Putting Tensors (and Models) on the GPU"
      ],
      "metadata": {
        "id": "33J_JhDR2Nvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason we want out tensors/models on the GPU is because using a GPU results in faster computations."
      ],
      "metadata": {
        "id": "30tpZCQ12ShU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor (default on the CPU)\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# Print the tensor and the device\n",
        "print(tensor, tensor.device) # tensor is initially on CPU\n",
        "\n",
        "# Move tensor to GPU (if available)\n",
        "tensor_on_gpu = tensor.to(device) # tensor is moved from CPU to GPU (if available)\n",
        "print(tensor_on_gpu, tensor_on_gpu.device)\n",
        "\n",
        "# Notes regarding \" device='cuda:0' \"\n",
        "# 'cuda:0': This is the index of the GPU that you're referring to. :0 means the first GPU. If your system has multiple GPUs,\n",
        "# they would be indexed as cuda:0, cuda:1, cuda:2, and so on."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DEa2C9e2f47",
        "outputId": "25c6445b-a6f5-4180-add9-66944bbd08d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]]) cpu\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]]) cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Moving Tensors Back to the CPU"
      ],
      "metadata": {
        "id": "jL2KJsBY35g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If tensor is on GPU, can't transform it to NumPy\n",
        "# To fix the tensor on GPU -> Numpy issue; we need to first set the tensor back to CPU before calling .numpy()\n",
        "tensor_on_cpu = tensor_on_gpu.cpu()\n",
        "print(tensor_on_cpu.numpy(), tensor_on_cpu.numpy().dtype)\n",
        "print('\\n', tensor_on_gpu)"
      ],
      "metadata": {
        "id": "yba2jURS4Axc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80df3102-743a-4957-da64-fabbb3ba011c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]] int64\n",
            "\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Miscellaneous\n"
      ],
      "metadata": {
        "id": "rQlIt0Bharsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Miscellaneous Notes - Image Tensor"
      ],
      "metadata": {
        "id": "eTWxoO1k0yFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT Prompt:** In tensor-speak (the language used to describe tensors), the tensor would have three dimensions, one for colour_channels, height and width.\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer:**\n",
        "\n",
        "The quote you've provided talks about how to describe the structure of a tensor, particularly in the context of image data, using the specific terminology or \"language\" associated with tensors.\n",
        "\n",
        "In the realm of machine learning and computer vision, images are commonly represented as tensors. A tensor, in this context, is essentially a multi-dimensional array. For an image, these dimensions correspond to different aspects of the data it contains:\n",
        "\n",
        "Color Channels: The number of channels in an image represents the depth of color information. For instance, a standard RGB (Red, Green, Blue) image has three color channels. Each channel stores intensity values for its respective color across the image. There are also grayscale images, which have just one channel, and other formats that may have more channels (like RGBA, where the A stands for Alpha or transparency).\n",
        "\n",
        "Height: This dimension represents the vertical size of the image in pixels.\n",
        "\n",
        "Width: This dimension represents the horizontal size of the image in pixels.\n",
        "\n",
        "So, when the quote mentions a tensor having \"three dimensions, one for color_channels, height, and width,\" it's describing how an image is structured in a way that's understood by tensor operations. For a color image in the RGB format, you could conceptualize it as a 3D tensor:\n",
        "\n",
        "The first dimension corresponds to the color channels (3 for RGB).\n",
        "The second dimension corresponds to the height of the image in pixels.\n",
        "The third dimension corresponds to the width of the image in pixels.\n",
        "Thus, if you have a 1080x1920 pixel RGB image, in tensor-speak, you would describe it as having dimensions of 3x1080x1920, where 3 represents the RGB channels, 1080 represents the height, and 1920 represents the width.\n",
        "\n",
        "Understanding this terminology is crucial for working with image data in machine learning frameworks like TensorFlow or PyTorch, as it affects how you process, manipulate, and train models with image data.\n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "\n",
        "Here's an example of what a tensor array for a 10x10 image with RGBA channels might look like. Each entry in this array is a randomly generated integer between 0 and 255, representing the intensity of the Red, Green, Blue, and Alpha (transparency) channels at each pixel:\n",
        "\n",
        "---\n",
        "```\n",
        "Array shape: (4, 10, 10)\n",
        "\n",
        "array([[[203, 178,  63, 203,  19,  80,  77,  99, 226, 227], #red\n",
        "        [148,  31, 141, 150, 187, 184, 214,  10,  40,  95],\n",
        "        [ 96,  76, 213, 117,  79, 224, 174,  44,   7, 236],\n",
        "        [182,  25, 242, 120, 207, 194,  52,  38,  42, 222],\n",
        "        [105, 193, 169, 146, 221,  76, 179,  84, 109, 222],\n",
        "        [  4, 105,  73,  54, 242, 172, 238,  23,  89, 192],\n",
        "        [ 79,  26,  75, 133,  92,  10, 115, 114,  87, 227],\n",
        "        [124, 183,  36, 176, 115, 246, 151, 227,  54,   1],\n",
        "        [  8, 255, 102, 242, 116,  93,  10, 184, 184,   1],\n",
        "        [161, 118,  98, 150,  89, 208,  76,  77,  58, 136]],\n",
        "\n",
        "       [[171, 231, 224, 128, 149, 101,  72, 167,  57, 178], #green\n",
        "        [170,   3, 175, 194, 243,  77, 188,  97, 197, 206],\n",
        "        [250, 197,  22, 191, 215, 156,  73,  38, 196,  34],\n",
        "        [255, 164, 187,  28, 114, 140,  65, 162,  74, 168],\n",
        "        [212, 127, 120, 162,  86,  22, 141, 126,  81, 167],\n",
        "        [168, 201,  21, 219, 148,  70, 102, 206, 111, 214],\n",
        "        [ 23, 204, 152,  56, 164, 110, 197, 213,  28, 206],\n",
        "        [162, 149, 100, 206,  25,  88, 161,  83, 173, 177],\n",
        "        [218,  38,  76, 220,  37,  36,  11, 228,   0, 107],\n",
        "        [ 29,  94, 173, 170,  31, 207, 251,  65, 205,  95]],\n",
        "\n",
        "       [[ 17, 117, 145,  64, 124,  20, 242, 202, 159,  72], #blue\n",
        "        [214, 102, 124, 170, 191,  93,  23, 251, 249,  62],\n",
        "        [ 25, 245, 169, 116, 131,  39,  68, 147,  79,  90],\n",
        "        [111,  33,  35, 196, 197, 170,  51, 197,  62,  70],\n",
        "        [ 47,  41, 113, 166, 202,  24, 242, 246,  68, 245],\n",
        "        [174,   4,  77,  83,  42,  65, 183,  58, 113,  44],\n",
        "        [221, 117, 252, 153, 119,  45, 137, 104,  58,  57],\n",
        "        [168, 231, 210, 239,  32,  30,   9, 150,  95, 214],\n",
        "        [127, 240,  56, 200, 161, 137, 197,   6, 231,  72],\n",
        "        [  4, 109,  53,  11, 197, 217,  88,   1, 249, 254]],\n",
        "\n",
        "       [[ 23, 252, 127,   8,  58, 222, 123,   6,  14, 108], #alpha\n",
        "        [  4, 195,  26, 177,  59, 145, 199, 167, 181,  93],\n",
        "        [  4, 119, 204, 182, 108,  53, 175, 159,  83,  50],\n",
        "        [ 47, 233, 103, 198,  84, 134,  42, 145,  93, 235],\n",
        "        [ 35, 165, 152,  77, 101, 153, 242, 206, 109,   9],\n",
        "        [ 78, 210, 182, 151,  48, 209, 199,  33, 138,  95],\n",
        "        [226, 206, 106,  47,  54,  49, 178, 107, 112, 105],\n",
        "        [195,  17,  24, 144,  91,  14, 102, 123, 246, 231],\n",
        "        [253, 137,  22, 133,  19, 192, 166,  24, 145,  78],\n",
        "        [ 69, 165, 135, 255, 122, 125, 135, 160, 174,  54]]], dtype=uint8)\n",
        "```\n",
        "---\n",
        "\n",
        "This representation shows a 4-dimensional tensor where:\n",
        "\n",
        "* The first dimension corresponds to the color channels (RGBA), so there are 4 layers or slices.\n",
        "* The second and third dimensions correspond to the height and width of the image, respectively, both set to 10 pixels in this example.\n",
        "\n",
        "Each \"layer\" or slice of the tensor represents one of the RGBA channels across the entire image. The values within each layer represent the intensity of that channel for each pixel, with 0 being no intensity (or completely transparent for the alpha channel) and 255 being the maximum intensity. ​​\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VwkYilIn02Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Jupyter notebooks, which include Google Colab notebooks, you do not always need to use the print() function to display output. This environment is designed for interactive computing, and it has a feature that automatically displays the result of the last operation in each cell.\n",
        "\n",
        "When you run a cell in a Jupyter notebook (or Google Colab, which is based on Jupyter), the following rules apply:\n",
        "\n",
        "The last line of code in a cell, when executed, will have its result displayed below the cell, even if it is not printed with the print() function. This is known as the cell's output.\n",
        "If the last line of code does not produce any output, nothing is displayed.\n",
        "If you have other lines of code in the cell whose output you wish to display, you need to explicitly use print() or another display function for those lines.\n",
        "This feature is especially useful for quick data exploration and analysis, as it allows you to see the results immediately without needing to use print() every time. However, if you want to format the output or display multiple outputs within a cell, you'll need to use print() or another suitable display function."
      ],
      "metadata": {
        "id": "Yj1fmRysZZ6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes Regarding Why you don't need to declare print() to output value in Google Colab**\n",
        "\n",
        "<br>\n",
        "\n",
        "In Jupyter notebooks, which include Google Colab notebooks, you do not always need to use the $print()$ function to display output. This environment is designed for interactive computing, and it has a feature that automatically displays the result of the last operation in each cell.\n",
        "\n",
        "When you run a cell in a Jupyter notebook (or Google Colab, which is based on Jupyter), the following rules apply:\n",
        "\n",
        "1. The last line of code in a cell, when executed, will have its result displayed below the cell, even if it is not printed with the $print()$ function. This is known as the cell's output.\n",
        "2. If the last line of code does not produce any output, nothing is displayed.\n",
        "3. If you have other lines of code in the cell whose output you wish to display, you need to explicitly use $print()$ or another display function for those lines.\n",
        "\n",
        "\n",
        "This feature is especially useful for quick data exploration and analysis, as it allows you to see the results immediately without needing to use $print()$ every time. However, if you want to format the output or display multiple outputs within a cell, you'll need to use $print()$ or another suitable display function."
      ],
      "metadata": {
        "id": "LaRNJIfGZdoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Miscellaneous Notes - Dimensions"
      ],
      "metadata": {
        "id": "Y_VYrwjwt_S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT Prompt**: what even is ndim in torch, as in, what is dimension of tensors even. as scalars are normally assosciated with the number line 1st dimension, however here it is assosicated with the 0th dimension. can i know why does tensorflow consider scalar as 0th, vector as 1st, matrix 2nd etc\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer**:\n",
        "\n",
        "The concept of dimensionality in tensors (as used in libraries like PyTorch and TensorFlow) is rooted in the mathematical definition of dimensions in linear algebra, but it's also slightly adapted to fit programming and data structure paradigms.\n",
        "\n",
        "**Understanding Dimensions**\n",
        "\n",
        "* Dimensionality: In the context of tensors, the dimensionality (or rank) refers to the number of indices required to uniquely select each element of the tensor. This concept is directly tied to how data can be organized and represented in structured forms:\n",
        "> * A scalar can be thought of as a single number. It doesn't contain any axes (or directions), so it's considered 0-dimensional. You don't need any index to access it because it's just a single value.\n",
        "> * A vector is a 1-dimensional array. You need one index to access each element. Think of it as a line where each element can be located at a certain position along that line.\n",
        "> * A matrix is a 2-dimensional array. You need two indices to access each element (one for the row and one for the column). It can be visualized as a plane or grid where positions are defined by two coordinates.\n",
        "> * Higher-dimensional tensors (3D, 4D, etc.) require more indices to access each element and represent more complex structures (like a cube for a 3D tensor).\n",
        "\n",
        "**Why Scalars are Considered 0-Dimensional**\n",
        "\n",
        "The designation of scalars as 0-dimensional in libraries like TensorFlow and PyTorch is consistent with how dimensions are mathematically defined. Since a scalar does not require any indices to access (it's just a single value), it is considered 0-dimensional. This definition helps maintain consistency across mathematical theory and its implementation in programming:\n",
        "\n",
        "* It aligns with the mathematical concept of a dimension as the number of coordinates needed to specify a point within it.\n",
        "* It provides a basis for defining operations and transformations on tensors in a way that is consistent regardless of their rank.\n",
        "\n",
        "**Practical Implications**\n",
        "\n",
        "This conceptual framework has practical implications in programming and data processing:\n",
        "\n",
        "* Generalization: By defining a scalar as a 0-dimensional tensor, vectors as 1-dimensional, matrices as 2-dimensional, and so on, libraries can offer a unified and consistent API for tensor operations that works across tensors of any dimensionality.\n",
        "* Broadcasting and Shape Manipulation: Understanding dimensionality is crucial for operations that involve multiple tensors, especially when their shapes differ. It allows for the implementation of broadcasting rules, where tensors of different shapes are used together in operations.\n",
        "* Data Representation: This hierarchical structure (scalar < vector < matrix < tensor) provides an intuitive way to represent increasingly complex data. For instance, a 3D tensor could represent a sequence of images (2D tensors), where each image is a matrix of pixel values.\n",
        "\n",
        "In summary, the classification of scalars as 0-dimensional and so on up the hierarchy to higher-dimensional tensors is a logical extension of mathematical principles into programming constructs, allowing for more versatile and powerful data manipulation capabilities in libraries like PyTorch and TensorFlow."
      ],
      "metadata": {
        "id": "MuZGa5V3uICX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Miscellaneous Notes - tensor.mean() Dtype 'Long' issue"
      ],
      "metadata": {
        "id": "ZQD8bpqOChsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT Prompt:** Why does long not work with .mean() in PyTorch\n",
        "\n",
        "**GPT Answer:** In PyTorch, the .mean() operation computes the arithmetic mean of all the elements in the tensor. For the mean to be meaningful, especially in the context of floating-point arithmetic, the operation is defined only for tensors with floating point or complex numbers. This is because the mean is not always an integer value, and rounding to an integer could lead to loss of precision and incorrect calculations in subsequent operations.\n",
        "\n",
        "The tensor you are working with has been created with the default integer type (torch.int64 or Long type in PyTorch), as inferred from the torch.arange function. To calculate the mean, you need to first convert it to a floating-point type. You can do this by using the .float() or .to() methods on the tensor.\n",
        "\n",
        "Here's how you can modify your tensor to compute the mean:\n",
        "\n",
        "```\n",
        "x_float = x.float()  # Convert x to a float tensor\n",
        "print(torch.mean(x_float), x_float.mean())  # Now you can compute the mean\n",
        "```\n",
        "\n",
        "This converts the tensor x to a floating point tensor (torch.float32), which allows the .mean() method to work correctly."
      ],
      "metadata": {
        "id": "dmh06M5UEgXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Miscellaneous Notes - Regarding Slicing [:] in Python"
      ],
      "metadata": {
        "id": "5dWcE3rWMski"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT Answer Regarding Slicing[:] in Python**\n",
        "In Python, slicing is a feature that allows you to access a portion of a sequence type (like lists, strings, tuples) or array-like objects (like NumPy arrays and PyTorch tensors). The slicing operator [:] is used to access elements of an iterable based on their indices. Here are some examples of how slicing works:\n",
        "\n",
        "---\n",
        "\n",
        "**Lists**\n",
        "\n",
        "* Let's start with a simple list:\n",
        "> `my_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`\n",
        "* Accessing all elements:\n",
        "> `all_elements = my_list[:]  # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`\n",
        "* Accessing the first 5 elements:\n",
        "> `first_five = my_list[:5]  # [0, 1, 2, 3, 4]`\n",
        "* Accessing elements from index 5 to the end:\n",
        "> `from_five_onwards = my_list[5:]  # [5, 6, 7, 8, 9]`\n",
        "* Accessing elements from index 3 to 7 (not including 7):\n",
        "> `subset = my_list[3:7]  # [3, 4, 5, 6]`\n",
        "* Accessing elements with a step (every second element):\n",
        "> `every_second = my_list[::2]  # [0, 2, 4, 6, 8]`\n",
        "* Accessing elements in reverse:\n",
        "> `reversed_list = my_list[::-1]  # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]`\n",
        "\n",
        "---\n",
        "\n",
        "**Strings**\n",
        "\n",
        "* Slicing works similarly with strings:\n",
        "> `my_string = \"Hello, World!\"`\n",
        "\n",
        "* Accessing all characters:\n",
        "> `all_chars = my_string[:]  # \"Hello, World!\"`\n",
        "\n",
        "* Accessing the first 5 characters:\n",
        "> `first_five = my_string[:5]  # \"Hello\"`\n",
        "\n",
        "* Accessing characters from index 7 to the end:\n",
        "> `from_seven_onwards = my_string[7:]  # \"World!\"`\n",
        "\n",
        "---\n",
        "\n",
        "**NumPy Arrays**\n",
        "\n",
        "* For NumPy arrays (and similarly for PyTorch tensors), slicing can be multidimensional:\n",
        "\n",
        "> ```\n",
        "import numpy as np\n",
        "my_array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "> ```\n",
        "\n",
        "* Accessing the entire array:\n",
        "> `all_elements = my_array[:]  # array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])`\n",
        "\n",
        "* Accessing the first two rows:\n",
        "> `first_two_rows = my_array[:2]  # array([[1, 2, 3], [4, 5, 6]])`\n",
        "\n",
        "* Accessing the second column:\n",
        "> `second_column = my_array[:, 1]  # array([2, 5, 8])`\n",
        "\n",
        "Accessing a submatrix (first two rows, last two columns):\n",
        "> `submatrix = my_array[:2, 1:]  # array([[2, 3], [5, 6]])`\n",
        "\n",
        "---\n",
        "\n",
        "Slicing is a powerful tool in Python that lets you retrieve elements efficiently from sequences and array-like objects without the need for explicit loops."
      ],
      "metadata": {
        "id": "dYvo2hNeVVti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Miscellaneous Notes - torch.stack() + torch.squeeze()\n",
        "\n"
      ],
      "metadata": {
        "id": "O0K3zfBbwa16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### torch.stack(), torch.vstack(), torch.hstack()\n",
        "\n",
        "GPT Prompt: Explain to me what does `torch.stack()` do?\n",
        "\n",
        "GPT Answer:\n",
        "\n",
        "`torch.stack()` is a function in PyTorch, a popular open-source machine learning library, that is used to concatenate a sequence of tensors along a new dimension. All tensors in the sequence **must have the same shape**. The function is particularly useful when you want to combine tensors in a way that creates an additional dimension, rather than simply joining them end-to-end as in concatenation.\n",
        "\n",
        "Here's a more detailed breakdown of its functionality:\n",
        "\n",
        "* Parameters: The `torch.stack(tensors, dim=0, *, out=None) -> Tensor` function takes several parameters, the most important of which are:\n",
        "\n",
        "  * tensors: a sequence of tensors to concatenate. These tensors must all have the same shape.\n",
        "  * dim: the dimension along which to concatenate the tensors. This is a new dimension that the tensors will be stacked along.\n",
        "  * out (Tensor, optional) – the output tensor.\n",
        "\n",
        "* Behavior: `torch.stack()` joins a sequence of tensors along a new dimension. All tensors in the sequence are expanded to have one additional dimension, and then they are joined along this new dimension. The position of this new dimension in the resultant tensor is specified by the dim parameter.\n",
        "\n",
        "* Result: The result of `torch.stack()` is a new tensor with one more dimension than the original tensors. For example, if you stack three 2D tensors (each of shape [2, 3]) along a new first dimension (`dim=0`), the resulting tensor will have the shape [3, 2, 3].\n",
        "\n",
        "Here's an example to illustrate its use:\n",
        "\n",
        " ```\n",
        "import torch\n",
        "\n",
        "# Creating two 2D tensors of shape [2, 3]\n",
        "tensor1 = torch.randn(2, 3)\n",
        "tensor2 = torch.randn(2, 3)\n",
        "\n",
        "# Stacking the two tensors along a new dimension (creates a new dimension at position 0)\n",
        "result = torch.stack((tensor1, tensor2), dim=0)\n",
        "\n",
        "print(result.shape)  # Output will be: torch.Size([2, 2, 3])\n",
        "```\n",
        "\n",
        "In this example, tensor1 and tensor2 are stacked along a new first dimension (dim=0), resulting in a tensor of shape [2, 2, 3], indicating that there are 2 tensors, each of shape [2, 3], stacked together.\n",
        "\n",
        "---\n",
        "GPT Prompt: Then what does param dim=-1 and dim=-2 do?\n",
        "\n",
        "GPT Answer:\n",
        "\n",
        "In PyTorch, `torch.stack()` is a function used to concatenate a sequence of tensors along a new dimension. All tensors in the sequence must have the same shape. The function creates a new tensor by stacking the input tensors, effectively adding one additional dimension to the resultant tensor, where each input tensor becomes a slice along the newly created dimension.\n",
        "\n",
        "The `dim` parameter in `torch.stack()` specifies the dimension along which the tensors are stacked. The dimensions are zero-indexed, meaning dim=0 would stack the tensors along a new first dimension, and the original dimensions of the tensors would follow after this new dimension.\n",
        "\n",
        "Regarding `dim=-1` and `dim=-2`:\n",
        "\n",
        "* `dim=-1` refers to the last dimension of the tensor. When you use dim=-1 in torch.stack(), it means you're stacking the tensors along the last dimension. After stacking, this newly created dimension becomes the new last dimension of the resultant tensor.\n",
        "\n",
        "* `dim=-2` refers to the second-to-last dimension of the tensor. Using dim=-2 in torch.stack() means the tensors are stacked along the second-to-last dimension, making the newly created dimension the new second-to-last dimension of the resultant tensor.\n",
        "\n",
        "These negative indices are useful for specifying dimensions relative to the end of the tensor, which can be especially handy when you're dealing with tensors of varying dimensions and you want to stack them along one of the last few dimensions without explicitly calculating or hard-coding the dimension index.\n",
        "\n",
        "---\n",
        "\n",
        "GPT Prompt: Then what is torch.vstack() and torch.hstack()\n",
        "\n",
        "GPT Answer:\n",
        "\n",
        "In PyTorch, torch.vstack() and torch.hstack() are functions used to concatenate tensors vertically and horizontally, respectively. These functions provide a convenient way to stack tensors along specific axes and are particularly useful when working with 2D tensors (matrices) but can also be applied to tensors with more dimensions. Here's a brief overview of each:\n",
        "\n",
        "`torch.vstack()`\n",
        "\n",
        "* **Vertical stacking (vstack):** This function stacks tensors vertically, which is equivalent to concatenating tensors along the first dimension (rows) for 2D tensors, or more generally, along a new first dimension for tensors of any shape. It's akin to stacking matrices on top of each other or appending rows to a matrix.\n",
        "* **Usage:** It's commonly used when you have tensors of the same width (i.e., the same size in all dimensions except for the first) and you want to combine them into a single tensor with greater height (more rows).\n",
        "\n",
        "`torch.hstack()`\n",
        "\n",
        "* **Horizontal stacking (hstack):** This function stacks tensors horizontally, equivalent to concatenating tensors along the second dimension (columns) for 2D tensors, or more generally, along the last dimension for 1D tensors. For tensors with more than two dimensions, it concatenates along the last dimension, which increases the size of that dimension.\n",
        "* **Usage:** It's used when you have tensors of the same height (i.e., the same size in all dimensions except for the second in the case of 2D tensors) and you want to combine them into a single tensor with greater width (more columns).\n",
        "\n",
        "These functions are particularly helpful in data manipulation and preprocessing tasks, such as combining data from different sources into a single dataset or reshaping data to fit the input requirements of a model."
      ],
      "metadata": {
        "id": "SSpMm-0nwnJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT Prompt:** Explain to me regarding torch.squeeze() and torch.unsqueeze()\n",
        "\n",
        "**GPT Answer:**\n",
        "\n",
        "In PyTorch, `torch.squeeze()` and `torch.unsqueeze()` are two functions used for modifying the dimensions of tensors, specifically for removing and adding dimensions of size 1, respectively. These functions are very useful for tensor shape manipulation, allowing for more flexible operations and easier integration with APIs expecting certain tensor shapes.\n",
        "\n",
        "<br>\n",
        "\n",
        "`torch.squeeze()`\n",
        "\n",
        "* Purpose: Removes all dimensions of size 1 from the tensor. If a specific dimension is given as an argument, torch.squeeze() removes the given dimension only if it is of size 1; otherwise, the tensor is returned unchanged.\n",
        "* Usage: This is useful when you want to eliminate redundant dimensions from your tensor, making your data more compact or ensuring it fits certain APIs that do not accept tensors with singleton dimensions. For example, if you have a tensor of shape (1, 3, 1, 5), applying squeeze() would reduce its shape to (3, 5).\n",
        "* Parameter:\n",
        "  * input (Tensor) – the input tensor.\n",
        "  * dim (int or tuple of ints, optional) – if given, the input will be squeezed only in the specified dimensions.\n",
        "* Syntax Example:\n",
        "> `squeezed_tensor = torch.squeeze(input_tensor, dim=None)`\n",
        "\n",
        "  Here, input_tensor is the tensor you want to squeeze, and dim is an optional argument specifying the dimension to squeeze. If dim is not provided, all dimensions of size 1 are removed.\n",
        "\n",
        "`torch.unsqueeze()`\n",
        "\n",
        "* Purpose: Adds a dimension of size 1 to the tensor at the specified dimension. This is the inverse operation of squeeze(), allowing you to expand the dimensions of your tensor.\n",
        "* Usage: This is particularly useful when you need to increase the dimensionality of a tensor for it to be compatible with certain operations that expect tensors of a certain shape. For instance, if you have a tensor of shape (3, 5) and you want to add a singleton dimension as the first dimension, you would end up with a shape of (1, 3, 5).\n",
        "* Syntax Example:\n",
        "> `unsqueezed_tensor = torch.unsqueeze(input_tensor, dim)`\n",
        "\n",
        "  Here, input_tensor is the tensor you want to unsqueeze, and dim is the position where the new dimension will be inserted.\n",
        "\n",
        "Both `squeeze()` and `unsqueeze()` are essential for shaping tensors in preparation for various operations, such as matrix multiplications, broadcasting, and feeding data into models, ensuring that tensors conform to the expected dimensions.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cCoqU4C1ywkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Miscellaneous Notes - torch.permute()\n"
      ],
      "metadata": {
        "id": "ZrINWfJI3r5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, `torch.permute()` is a function used to rearrange the dimensions of a tensor according to a specified order. This allows for complex reshaping and reordering of the data within a tensor, making it a powerful tool for manipulating tensor shapes and dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "How `torch.permute()` Works\n",
        "\n",
        "* Purpose: To reorder the dimensions of a tensor.\n",
        "* Usage: When you want to change the order of axes/dimensions of a tensor. For example, if you have a tensor of shape (2, 3, 4) and you want to change the order of dimensions to make it (3, 4, 2), you can use torch.permute() to achieve this.\n",
        "\n",
        "---\n",
        "\n",
        "Syntax and Example\n",
        "```\n",
        "import torch\n",
        "\n",
        "# Creating a tensor of shape (2, 3, 4)\n",
        "x = torch.randn(2, 3, 4)\n",
        "\n",
        "# Permute the dimensions\n",
        "# Here, we want the dimension order to be (1, 2, 0),\n",
        "# meaning the new shape will be (3, 4, 2)\n",
        "y = x.permute(1, 2, 0)\n",
        "\n",
        "print(\"Original shape:\", x.shape)\n",
        "print(\"New shape:\", y.shape)\n",
        "```\n",
        "\n",
        "In the permute(1, 2, 0) call, the arguments represent the new order of the dimensions:\n",
        "\n",
        "* The first dimension (0) of x becomes the third dimension of y.\n",
        "* The second dimension (1) of x becomes the first dimension of y.\n",
        "* The third dimension (2) of x becomes the second dimension of y.\n",
        "\n",
        "This results in y having a shape of (3, 4, 2), as per the example.\n",
        "\n",
        "**Key Points**\n",
        "\n",
        "* `torch.permute()` does not change the data within the tensor; it only changes how the data is organized by rearranging the tensor dimensions.\n",
        "* It's useful for operations that require specific dimension orders, such as when preparing data for convolutional layers in neural networks, where you might need to permute dimensions to match the expected input shape.\n",
        "\n",
        "This flexibility makes `torch.permute()` an essential function for tensor manipulation, especially in complex data processing pipelines and neural network operations."
      ],
      "metadata": {
        "id": "fJXz4z5t3xNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Miscellaneous Notes - cuda-semantics\n",
        "\n",
        "URL: https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics"
      ],
      "metadata": {
        "id": "h0uwxrELYKf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensor Allocation + CUDA Devices\n"
      ],
      "metadata": {
        "id": "9lZ2df4NY7Y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT PROMPT:**\n",
        "\n",
        ">regarding pytorch.cuda, what does the below mean\n",
        ">\n",
        ">\"However, once a tensor is allocated, you can do operations on it irrespective of the selected device, and the results will be always placed on the same device as the tensor.\"\n",
        ">\n",
        ">because i thought that to do operations on tensor which is already on the GPU's ram, you have to bring it back to the CPU before conducting operations?\n",
        "\n",
        "---\n",
        "\n",
        "**GPT ANSWER:**\n",
        "\n",
        "The statement you're referring to actually highlights a key feature of PyTorch's handling of tensors on different devices (like CPUs and GPUs). Here's a breakdown of what it means:\n",
        "\n",
        "- **Tensor Allocation**: When you create a tensor in PyTorch, you can specify the device it resides on. This could be the CPU or a GPU. For example, if you allocate a tensor on a GPU, it's stored in the GPU's memory.\n",
        "\n",
        "- **Operations on Tensors**: Once a tensor is allocated on a device (let's say a GPU for this discussion), you can perform operations on it directly on that device. There's no need to move it back to the CPU for processing. PyTorch is designed to execute these operations on the device where the tensor resides. This is done for efficiency and performance reasons, as moving data between devices (especially between CPU and GPU) can be costly in terms of time and resources.\n",
        "\n",
        "- **Result Placement**: After performing an operation on a tensor, the result of that operation will automatically be placed on the same device as the tensor. For example, if you have a tensor `A` on GPU and you add another tensor `B` to it (assuming `B` is also on the GPU or is broadcastable/compliant with `A`'s dimensions), the result of `A + B` will also be on the GPU.\n",
        "\n",
        "This means you don't have to manually move tensors between devices for most operations. PyTorch handles these operations efficiently within the device's own memory (RAM in the case of CPUs, VRAM for GPUs), avoiding unnecessary data transfers.\n",
        "\n",
        "It's a common misconception that tensors must be moved back to the CPU for operations. In reality, PyTorch and other modern deep learning frameworks are designed to perform tensor operations directly on the device where the tensors are located, leveraging the computational power of GPUs for faster processing without the bottleneck of data transfer between CPU and GPU."
      ],
      "metadata": {
        "id": "8mPNOQiRYQRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick Note:\n",
        "* Default CUDA - default GPU\n",
        "* cuda:0 is GPU 1\n",
        "* cuda:1 is GPU 2...\n",
        "\n",
        "Code source: https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics\n",
        "\n",
        "More Notes;\n",
        "* `torch.cuda` is used to set up and run CUDA operations. It keeps track of the currently selected GPU, and all CUDA tensors you allocate will by default be created on that device.\n",
        "* Understanding as of now, to declare for tensor to be allocated on GPU, need to write `tensor(arr, device=cuda-ith-GPU)` or `tensor(arr).cuda()`\n",
        "* `with torch.cuda.device(1):` - this line is referring to if your attempting to switch to a GPU with index 1, which implies a second GPU. (won't work on Colab as only have 1x GPU at most).\n",
        "* Pay attention how tensor\n",
        "\n",
        "\n",
        "```\n",
        "cuda = torch.device('cuda')     # Default CUDA device\n",
        "cuda0 = torch.device('cuda:0')\n",
        "cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n",
        "\n",
        "x = torch.tensor([1., 2.], device=cuda0)\n",
        "# x.device is device(type='cuda', index=0)\n",
        "y = torch.tensor([1., 2.]).cuda()\n",
        "# y.device is device(type='cuda', index=0)\n",
        "\n",
        "with torch.cuda.device(1):\n",
        "    # allocates a tensor on GPU 1\n",
        "    a = torch.tensor([1., 2.], device=cuda)\n",
        "\n",
        "    # transfers a tensor from CPU to GPU 1\n",
        "    b = torch.tensor([1., 2.]).cuda()\n",
        "    # a.device and b.device are device(type='cuda', index=1)\n",
        "\n",
        "    # You can also use ``Tensor.to`` to transfer a tensor:\n",
        "    b2 = torch.tensor([1., 2.]).to(device=cuda)\n",
        "    # b.device and b2.device are device(type='cuda', index=1)\n",
        "\n",
        "    c = a + b\n",
        "    # c.device is device(type='cuda', index=1)\n",
        "\n",
        "    z = x + y\n",
        "    # z.device is device(type='cuda', index=0)\n",
        "\n",
        "    # even within a context, you can specify the device\n",
        "    # (or give a GPU index to the .cuda call)\n",
        "    d = torch.randn(2, device=cuda2)\n",
        "    e = torch.randn(2).to(cuda2)\n",
        "    f = torch.randn(2).cuda(cuda2)\n",
        "    # d.device, e.device, and f.device are all device(type='cuda', index=2)\n",
        "  ```\n",
        "\n",
        "  In the PyTorch framework, when you are specifying devices like this, you're telling PyTorch where you want your tensors to be stored and computed. Here's what each of the lines means:\n",
        "\n",
        "1. `cuda = torch.device('cuda')`:\n",
        "\n",
        "* This line creates a device object that represents the default CUDA device. In systems with multiple GPUs, PyTorch automatically selects the default GPU based on your CUDA setup or any environmental variables you might have set. If you simply specify 'cuda' without an index, PyTorch uses the default GPU. This is useful when you don't want to hardcode your GPU selection in your script but instead rely on the system's default choice.\n",
        "\n",
        "2. `cuda0 = torch.device('cuda:0')`:\n",
        "\n",
        "* This creates a device object pointing specifically to the first GPU of your system. GPUs are 0-indexed, so 'cuda:0' refers to the first GPU. When you use this device object to move tensors or create new tensors, you're explicitly telling PyTorch to use the first GPU.\n",
        "\n",
        "3. `cuda2 = torch.device('cuda:2')`:\n",
        "\n",
        "* Similarly, this line is creating a device object for what is labeled as the third GPU in your system (again, because of 0-indexing: 0, 1, 2, ...). This is specifying that you want to use the third GPU for your tensors and computations. This line assumes that you have at least three GPUs in your system. If you don't have a third GPU (i.e., if you have fewer than three GPUs), attempting to use cuda2 for operations will result in an error because the specified device does not exist.\n",
        "\n",
        "Each of these device objects (cuda, cuda0, cuda2) can be used with tensors and PyTorch operations to control on which GPU (or whether on any GPU at all, in the case of the default cuda device) those operations are executed. This is particularly useful in multi-GPU setups where you might want to distribute your computation across different GPUs or ensure specific operations run on specific GPUs for efficiency or organizational reasons."
      ],
      "metadata": {
        "id": "MlA-voJSZd0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch Docs -- Cuda Semantics\n"
      ],
      "metadata": {
        "id": "7_DatbmFS-6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**MUST READS:**\n",
        "\n",
        "* Info on RNN, CNN, GAN, AutoEncoder+: https://www.techtarget.com/searchenterpriseai/feature/CNN-vs-RNN-How-they-differ-and-where-they-overlap\n",
        "\n",
        "* torch.docs, cuda semantics URL: https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics\n",
        "\n",
        "* Latent Space Introduction: https://samanemami.medium.com/a-comprehensive-guide-to-latent-space-9ae7f72bdb2f\n",
        "\n",
        "* VAE: https://ai.plainenglish.io/what-is-a-variational-autoencoder-vae-0a8d337810f8\n",
        "\n",
        "* Deep One-Class Classification: https://medium.com/analytics-vidhya/paper-summary-deep-one-class-classification-doc-adc4368af75c"
      ],
      "metadata": {
        "id": "0D9b0RayjsPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.rand() vs torch.randn()**\n",
        "\n",
        "* In summary, randn generates numbers from a normal distribution with a mean of 0 and a standard deviation of 1, while `rand` generates numbers from a uniform distribution between 0 and 1.\n",
        "\n",
        "---\n",
        "\n",
        "**What is FP16 GEMM**\n",
        "\n",
        "* FP16 GEMM refers to General Matrix Multiply operations performed using half-precision floating-point arithmetic, which uses 16-bit floating-point numbers (FP16). These operations are common in deep learning and other applications requiring high performance computing. Using FP16 can lead to faster computation and reduced memory usage compared to full precision (FP32) operations, at the potential cost of reduced numerical precision. This can be beneficial on hardware like GPUs that are optimized for lower precision arithmetic, allowing for more operations per second. The benchmarks shown in the image are likely comparing the performance of matrix multiplication operations using FP16 with and without reduced precision optimizations.\n",
        "\n",
        "---\n",
        "\n",
        "**Machine learning inference Definition**\n",
        "\n",
        "* Machine learning inference is the process of running data points into a machine learning model to calculate an output such as a single numerical score. This process is also referred to as \"operationalizing a machine learning model\" or \"putting a machine learning model into production.\"\n",
        "\n",
        "---\n",
        "\n",
        "**Spatial Transformer Network (STN)**\n",
        "\n",
        "* Spatial transformer networks (STN for short) allow a neural network to learn how to perform spatial transformations on the input image in order to enhance the geometric invariance of the model. For example, it can crop a region of interest, scale and correct the orientation of an image.\n",
        "* Spatial Transformer Networks (STN) is a deep learning technique that allows for the automatic spatial manipulation and transformation of data within a neural network.\n",
        "\n",
        "---\n",
        "\n",
        "**GPT Question:** explain what all of these are: {nn.Linear, nn.Conv*, cdist, tensordot, affine grid and grid sample, adaptive log softmax, GRU and LSTM}\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer:**\n",
        "Certainly, the terms you've listed are associated with neural networks and their operations, particularly in the context of PyTorch, a popular machine learning library for Python.\n",
        "\n",
        "1. **nn.Linear**: This refers to a linear transformation to the incoming data, which is also known as a fully connected or dense layer. It applies a linear function `(y = xA^T + b)` that transforms the input `x` using learned weights `A` and biases `b`.\n",
        "\n",
        "2. **nn.Conv***: This is a prefix for convolutional layers in PyTorch, where the asterisk (*) can stand for different dimensions of convolutional operations like `nn.Conv1d` for 1-dimensional convolutions, `nn.Conv2d` for 2-dimensional convolutions (commonly used for image data), and `nn.Conv3d` for 3-dimensional convolutions (often used for volumetric data).\n",
        "\n",
        "3. **cdist**: This function computes the pairwise distances between two sets of vectors, often used to measure the similarity or dissimilarity between the sets. In PyTorch, `torch.cdist` calculates the distances using various metrics like Euclidean, Manhattan, or cosine similarity.\n",
        "\n",
        "4. **tensordot**: A tensordot operation computes tensor dot products along specified axes. This is a generalized matrix multiplication that allows for higher dimensional tensor products, which is useful in multidimensional array computations.\n",
        "\n",
        "5. **affine grid and grid sample**: These are functions often used together for spatial transformer networks. `affine_grid` generates a grid of coordinates that correspond to the input data and can be used to apply affine transformations. `grid_sample` then uses this grid to sample the input data, performing tasks like cropping, scaling, rotating, and translation, enabling the network to spatially transform the input image in a differentiable manner.\n",
        "\n",
        "6. **adaptive log softmax**: This is an implementation of a softmax variant that is more computationally efficient for cases where the number of classes is extremely large. It adaptively uses fewer computations for less frequent targets and more for frequent ones, providing a trade-off between computational complexity and precision.\n",
        "\n",
        "7. **GRU and LSTM**:\n",
        "   - **GRU (Gated Recurrent Unit)**: A type of recurrent neural network architecture that is similar to an LSTM but has fewer parameters. It combines the forget and input gates into a single \"update gate\" and merges the cell state and hidden state.\n",
        "   - **LSTM (Long Short-Term Memory)**: Another recurrent neural network architecture that is designed to better capture long-term dependencies compared to standard RNNs. LSTMs include mechanisms called gates to control the flow of information: these are the forget gate, input gate, and output gate. They have an internal state that helps them capture long-term dependencies in sequential data.\n",
        "\n",
        "Both GRUs and LSTMs are commonly used in sequence modeling tasks such as language modeling, time-series prediction, and more. They are designed to address the vanishing gradient problem that can occur with standard RNNs when dealing with long sequences.\n",
        "\n"
      ],
      "metadata": {
        "id": "TxQcAEidj8ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Learning ++ Dimensionality Reduction (Definition)**\n",
        "\n",
        "* Feature learning, in the context of machine learning, is the automatic process through which a model identifies and optimizes key patterns, structures, or characteristics (called \"features\") from raw data to enhance its performance in a given task.\n",
        "\n",
        "* Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension.\n",
        "\n",
        "---\n",
        "**Latent Space + Brief Autoencoder**\n",
        "\n",
        "* URL: https://samanemami.medium.com/a-comprehensive-guide-to-latent-space-9ae7f72bdb2f\n",
        "* Definition Quick Peek: Latent space is a lower-dimensional space that captures the essential features of the input data. In simpler terms, it is a compressed representation of the original data where each dimension corresponds to a specific feature or characteristic. This dimensionality reduction is achieved through various techniques, such as autoencoders and variational autoencoders (VAEs), which learn to encode the most important information in the data.\n",
        "\n",
        "---\n",
        "\n",
        "**VAE - Variational Autoencoder**\n",
        "\n",
        "* https://ai.plainenglish.io/what-is-a-variational-autoencoder-vae-0a8d337810f8\n",
        "* Brief intro: Like GANs, Variational Autoencoders (VAEs) can be used for this purpose. Being an adaptation of classic autoencoders, which are used for dimensionality reduction and input denoising, VAEs are generative. Unlike the classic ones, with VAEs you can use what they’ve learned in order to generate new samples. Blends of images, predictions of the next video frame, synthetic music — the list goes on.\n",
        "\n",
        "---\n",
        "\n",
        "**GPT - Autoencoders**\n",
        "\n",
        "An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data, usually for the purpose of dimensionality reduction or feature learning. The main goal of an autoencoder is to compress the input data into a latent-space representation and then reconstruct the output from this representation to match the original input as closely as possible.\n",
        "\n",
        "#### Structure of an Autoencoder:\n",
        "\n",
        "An autoencoder typically consists of two main parts:\n",
        "\n",
        "1. **Encoder**: This part of the network compresses the input into a latent-space representation. It encodes the high-dimensional data into a lower-dimensional code. The encoder layer(s) process the input data to reduce its dimensionality, usually through a series of linear transformations and non-linear activations.\n",
        "\n",
        "2. **Decoder**: This part attempts to reconstruct the input from the latent space representation. The decoder architecture is typically symmetrical to the encoder, facilitating the learning of the reverse operation from the compressed code back to the original dimension of the data.\n",
        "\n",
        "#### Working of an Autoencoder:\n",
        "\n",
        "- **Encoding**: The input data is passed through the encoder, which is a feed-forward neural network, and it gets transformed into the latent space (code).\n",
        "- **Code**: The code is a compact \"summary\" or \"compression\" of the input data, containing the essential information needed to reconstruct it.\n",
        "- **Decoding**: The decoder takes this code and reconstructs the input data as closely as possible. This reconstruction can be lossy (some original information might be lost).\n",
        "- **Reconstruction Loss**: The difference between the original input and the reconstructed input is calculated using a loss function (such as mean squared error). The autoencoder learns to minimize this reconstruction loss during training.\n",
        "\n",
        "### Variations and Uses:\n",
        "\n",
        "Autoencoders can be used for various purposes:\n",
        "\n",
        "- **Dimensionality Reduction**: Similar to PCA, autoencoders can reduce the dimensionality of the data by learning a reduced set of features (encoding), which can then be used for tasks like visualization.\n",
        "\n",
        "- **Feature Learning**: Autoencoders can learn a representation (encoding) for the data that can be useful for tasks like classification.\n",
        "\n",
        "- **Denoising**: When trained to ignore \"noise,\" autoencoders can learn to recover the underlying structure of the data and can be used to remove noise from signals.\n",
        "\n",
        "- **Anomaly Detection**: By learning the normal pattern of input data, autoencoders can detect outliers by evaluating the reconstruction loss.\n",
        "\n",
        "- **Generative Models**: Variations like Variational Autoencoders (VAEs) can generate new data that's similar to the input data.\n",
        "\n",
        "In all these cases, the key concept is that the autoencoder learns a representation (encoding) for the set of data, typically for the purpose of dimensionality reduction or pattern recognition.\n",
        "\n",
        "---\n",
        "\n",
        "**Non-Linear Activation Functions**\n",
        "\n",
        "* URL (12 types of Activation Functions): https://www.v7labs.com/blog/neural-networks-activation-functions#:~:text=Non%2Dlinear%20activation%20functions%20solve,can%20provide%20a%20better%20prediction\n",
        "* Non-linear activation functions solve the following limitations of linear activation functions: They allow backpropagation because now the derivative function would be related to the input, and it's possible to go back and understand which weights in the input neurons can provide a better prediction.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "duVbU6PxvQNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes for \"12 types of Activation Functions\"\n"
      ],
      "metadata": {
        "id": "uxZ8Wz8wS6Pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Binary Step Function\n"
      ],
      "metadata": {
        "id": "hJXgjgeV-0sh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-class Classification**\n",
        "\n",
        "* Multi-class classification problems are a type of machine learning problem where the goal is to classify instances into one of three or more classes. Unlike binary classification, where there are only two possible outcomes, multi-class classification deals with situations where an instance could belong to any one of multiple categories.\n",
        "\n",
        "* For example, a multi-class classification problem could involve classifying types of fruit in a basket into categories such as apples, bananas, cherries, and oranges. Each fruit must be classified into one of these categories, and there are more than two categories to choose from.\n",
        "\n",
        "* The binary step function shown in your image is not suitable for multi-class classification because it is designed to provide only two possible outputs: 0 or 1. This limitation means it can only differentiate between two classes (binary classification) and not more. For multi-class problems, other functions and algorithms are needed that can output a probability distribution over multiple classes, like the softmax function commonly used in neural networks for multi-class classification tasks.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Gradient 0 -- Binary Step Function -- Backpropagation**\n",
        "\n",
        "**GPT Prompt:** The gradient of the step function is zero, which causes a hindrance in the backpropagation process.\" // why is the gradient 0? and how is this a hindrance to backpropagation\n",
        "\n",
        "**GPT Answer:**\n",
        "\n",
        "* The gradient of a function at a point measures how much the output of the function changes if the input is changed slightly. For a binary step function, which graphically looks like a step (it is constant at 0 for all values below 0 and constant at 1 for all values equal to or above 0), there is no change in the output value as the input changes, except exactly at the point where the step occurs (x = 0). However, the gradient at a point is defined as the limit of the slope of the function as the change in the input approaches zero, and for a step function, this limit is zero everywhere except at the step itself, where it is undefined.\n",
        "\n",
        "* Mathematically, the derivative (or gradient) of the constant function is zero because there is no rate of change — the function is flat. Since the binary step function is composed of two constant functions, its gradient is zero everywhere except at the point of discontinuity (x = 0), where it technically does not have a derivative because the function jumps from one value to another without passing through intermediate values.\n",
        "\n",
        "* In the context of neural networks, the backpropagation process relies on the gradient of the activation function to update the weights. This process is what allows the network to learn from data and improve its predictions over time. The gradient indicates the direction in which the weights need to be adjusted to minimize the error of the network's output.\n",
        "\n",
        "* However, if the gradient is zero, as is the case with a binary step function, there is no direction for the backpropagation algorithm to follow — it suggests that no change to the weights will decrease the error, which is not helpful for learning. This is why the zero gradient (or flat gradient) is a hindrance to backpropagation: it provides no information on how to adjust the weights to improve the model, thus \"hindering\" the learning process.\n",
        "\n",
        "* This is a major reason why smoother activation functions like sigmoid or tanh, which have non-zero gradients across most of their domain, or ReLU and its variants, which have non-zero gradients for positive inputs, are preferred in practice for neural networks. These functions provide gradients that facilitate weight updates and enable effective learning through backpropagation.\n"
      ],
      "metadata": {
        "id": "rkLevCPVS4Mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear Functions vs Non-linear Functions (sigmoid, tanh)\n",
        "\n",
        "* URL of Activation Functions: https://www.techtarget.com/searchenterpriseai/feature/CNN-vs-RNN-How-they-differ-and-where-they-overlap\n",
        "* URL of GPT Chat History: https://chat.openai.com/c/dd7ad124-4421-4451-ad72-7da4d830d555\n",
        "* Refer to Desmos graph on Sigmoid, Sigmoid MSE lost function, Tanh: https://www.desmos.com/calculator/qswof9l3y6"
      ],
      "metadata": {
        "id": "RvC2XL44-1vU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT Prompt:** \"It’s not possible to use backpropagation as the derivative of the function is a constant and has no relation to the input x. \" // why is there no relation // (This is regarding Linear Activation Functions, e.g. $f(x) = x$)\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer:**\n",
        "\n",
        "The statement \"It's not possible to use backpropagation as the derivative of the function is a constant and has no relation to the input x\" from the image you uploaded is slightly misleading. Let me clarify.\n",
        "\n",
        "In the context of neural networks, an activation function's derivative is important because it is used during the backpropagation algorithm to update the weights in proportion to the error gradient. For a linear activation function, like $f(x) = x$, the derivative is a constant 1, because the rate of change of the function with respect to x is constant and does not depend on the value of x.\n",
        "\n",
        "The derivative $df/dx$ of $f(x) = x$ is $1$ for all $x$. This means that the gradient will not change based on the input value. In the context of backpropagation, this has a couple of implications:\n",
        "\n",
        "1. **Lack of Error Scaling:** The gradient will be the same regardless of the input, which means the change made to weights during backpropagation will not take into account how far off the predictions are. All errors, regardless of their magnitude, will result in the same size of the weight update, which is not ideal because large errors typically should lead to larger adjustments.\n",
        "\n",
        "2. **No Non-linearity:** When using a linear activation function in all layers of a neural network, the entire network remains a linear model. This is because a stack of linear layers can only represent linear functions; the composition of two linear functions is still a linear function. One of the strengths of neural networks is their ability to capture non-linear relationships, and with linear activation functions, this capability is lost.\n",
        "\n",
        "To summarize, it's not that backpropagation can't be used with a linear activation function; rather, the use of a linear activation function throughout a neural network results in a model that fails to capture non-linear relationships and may not benefit from the stacked layers of a deep network. Non-linear activation functions allow the network to learn more complex patterns in the data.\n",
        "\n",
        "---\n",
        "\n",
        "**GPT Prompt:** regadring the dL/dw and dL/db part, compare for me between linear and non-linear activation function, and utilise example etc to proof for me as to your previous arguments and why linear is unsuitable etc\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer:**\n",
        "Let's discuss the mathematical underpinnings of backpropagation with linear and non-linear activation functions in a neural network, focusing on the gradients of the loss with respect to weights $( \\frac{\\partial L}{\\partial w} $) and biases $( \\frac{\\partial L}{\\partial b} $), which are crucial for learning during the training process.\n",
        "\n",
        "**Linear Activation Function:**\n",
        "\n",
        "A linear activation function can be defined as $ f(x) = x $. In this case, the derivative with respect to $ x $ is constant (which is $ 1 $ in this case). This means that the gradient of the activation function does not depend on the input $ x $.\n",
        "\n",
        "For a single-layer neural network with a linear activation function, the output $ y $ for a given input\n",
        "$ x $ is:\n",
        "\n",
        "$ y = w \\cdot x + b $\n",
        "\n",
        "Where $ w $ is the weight and $ b $ is the bias. The derivative of the loss $ L $ with respect to the weight $ w $ $( \\frac{\\partial L}{\\partial w} $) is proportional to the input $ x $ (assuming a simple loss function like mean squared error), and the derivative with respect to the bias $( \\frac{\\partial L}{\\partial b} $) is a constant.\n",
        "\n",
        "If we had multiple layers, all with linear activations, the output of the network would still be a linear function of the input, because the composition of linear functions is itself a linear function. No matter how many layers you stack, the model's capacity is equivalent to that of a single-layer network, and it cannot capture complex patterns in the data that are inherently non-linear.\n",
        "\n",
        "**Non-Linear Activation Function:**\n",
        "\n",
        "Now let's consider a non-linear activation function like sigmoid, defined as:\n",
        "\n",
        "$ f(x) = \\frac{1}{1 + e^{-x}} $\n",
        "\n",
        "The derivative of this function with respect to $ x $ (which is $ f'(x) = f(x) \\cdot (1 - f(x)) $) varies depending on the value of $ x $, which allows for a more dynamic adjustment of the weights and biases during training. This variability is crucial because it allows the network to adjust the weights differently depending on the magnitude of the error.\n",
        "\n",
        "For a single-layer neural network with a sigmoid activation function, the output $ y $ for a given input $ x $ is:\n",
        "\n",
        "$ y = \\frac{1}{1 + e^{-(w \\cdot x + b)}} $\n",
        "\n",
        "Here, the gradient of the loss with respect to the weight $( \\frac{\\partial L}{\\partial w} $) and the gradient with respect to the bias $( \\frac{\\partial L}{\\partial b} $) are now influenced by the derivative of the sigmoid function. So, the updates to the weights will now take into account the actual output of the activation function and its gradient. If the network's prediction is very wrong, the adjustments will be significant; if the prediction is close to the target, the adjustments will be smaller.\n",
        "\n",
        "**Lack of Error Scaling in Linear Functions:**\n",
        "\n",
        "In the linear case, because the derivative with respect to $ w $ is always proportional to $ x $ (and hence independent of the activation function), the backpropagation algorithm will update all weights by the same factor in each layer, regardless of the input value. This means that it doesn't take into account the 'scale' of the error. In contrast, with non-linear activation functions, the derivative with respect to $ w $ includes terms from the activation function's derivative, which varies with $x$, allowing the network to make smaller or larger updates depending on how confident it is about its predictions.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Imagine a scenario where you're training a neural network to predict house prices. Your features might include the number of rooms, location score, and age of the building. A linear model might just learn to assign weights to these features and add them up to get a price. But if house prices increase non-linearly with the number of rooms (say, due to market perceptions of houses with more than 3 rooms), a linear activation cannot model this pattern, and therefore, the network will not learn it regardless of how you adjust the weights. A non-linear activation function, however, can capture this non-linear relationship, allowing for much more nuanced and accurate predictions.\n",
        "\n",
        "---\n",
        "\n",
        "**GPT Prompt:** now just compare examples, and show me why cant the loss function of linear capture what the MSE of non-linear can. as in just show me examples of big and small changes for both type of activations, and show me why the delta_b and delta_w is not øptimal for linear but optimal for non-linear activation functions\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer:**\n",
        "Let's compare how the updates to weights $Δw$ and biases $Δb$ behave when using mean squared error $MSE$ as the loss function for both linear and non-linear activation functions.\n",
        "\n",
        "We will consider a simple scenario: suppose we have a single neuron with a single input. The goal of the neuron is to learn to predict some output given this input.\n",
        "\n",
        "The mean squared error loss function is given by:\n",
        "$ L = \\frac{1}{2}(y_{\\text{true}} - y_{\\text{pred}})^2 $\n",
        "Where $ y_{\\text{true}} $ is the true output and $ y_{\\text{pred}} $ is the predicted output by the neural network.\n",
        "\n",
        "**For a Linear Activation Function:**\n",
        "\n",
        "The output of the neuron is a linear function of the input:\n",
        "$ y_{\\text{pred}} = w \\cdot x + b $\n",
        "\n",
        "The partial derivatives of the loss with respect to weight $w$ and bias $b$ are:\n",
        "$ \\frac{\\partial L}{\\partial w} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot x $\n",
        "$ \\frac{\\partial L}{\\partial b} = (y_{\\text{pred}} - y_{\\text{true}}) $\n",
        "\n",
        "**Example with Linear Activation:**\n",
        "\n",
        "- Case 1: Large error (e.g., $ y_{\\text{true}} = 10, y_{\\text{pred}} = 0 $)\n",
        "  - $ \\frac{\\partial L}{\\partial w} = (0 - 10) \\cdot x = -10x $\n",
        "  - $ \\frac{\\partial L}{\\partial b} = (0 - 10) = -10 $\n",
        "\n",
        "- Case 2: Small error (e.g., $ y_{\\text{true}} = 10, y_{\\text{pred}} = 9 $)\n",
        "  - $ \\frac{\\partial L}{\\partial w} = (9 - 10) \\cdot x = -x $\n",
        "  - $ \\frac{\\partial L}{\\partial b} = (9 - 10) = -1 $\n",
        "\n",
        "Regardless of the size of the error, the changes to the weights and biases are just scaled versions of the input and do not consider the context of the error's size beyond that scaling.\n",
        "\n",
        "**For a Non-Linear Activation Function:**\n",
        "\n",
        "Now let's assume the activation function is sigmoid, so the output becomes:\n",
        "$ y_{\\text{pred}} = \\frac{1}{1 + e^{-(w \\cdot x + b)}} $\n",
        "\n",
        "The partial derivatives of the loss with respect to w and b become more complex due to the chain rule:\n",
        "$ \\frac{\\partial L}{\\partial w} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot y_{\\text{pred}} \\cdot (1 - y_{\\text{pred}}) \\cdot x $\n",
        "$ \\frac{\\partial L}{\\partial b} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot y_{\\text{pred}} \\cdot (1 - y_{\\text{pred}}) $\n",
        "\n",
        "**Example with Non-Linear Activation:**\n",
        "\n",
        "- Case 1: Large error (e.g., $ y_{\\text{true}} = 1, y_{\\text{pred}} = \\text{sigmoid}(-10) \\approx 0 $)\n",
        "  - The derivative $ y_{\\text{pred}} \\cdot (1 - y_{\\text{pred}}) $ will be significant because sigmoid(-10) is close to 0, so its derivative is close to its maximum value.\n",
        "  - $ \\frac{\\partial L}{\\partial w} $ and $ \\frac{\\partial L}{\\partial b} $ will be large, leading to a significant weight update.\n",
        "\n",
        "- Case 2: Small error (e.g., $ y_{\\text{true}} = 1, y_{\\text{pred}} = \\text{sigmoid}(0.9) \\approx 0.71 $)\n",
        "  - The derivative $ y_{\\text{pred}} \\cdot (1 - y_{\\text{pred}}) $ will be smaller because sigmoid(0.9) is closer to 1, and its derivative is smaller.\n",
        "  -  $\\frac{\\partial L}{\\partial w} $ and $ \\frac{\\partial L}{\\partial b} $ will be smaller, leading to a more modest weight update.\n",
        "\n",
        "In the non-linear case, the gradient not only scales with the input but also with the derivative of the activation function, which is dependent on the current output of the neuron. This means that the network can make smaller adjustments when the prediction is close to the target and larger adjustments when the prediction is far off.\n",
        "\n",
        "\n",
        "\n",
        "**Why is Linear Activation Unsuitable?**\n",
        "\n",
        "The linear activation function's lack of sensitivity to the size of the error (i.e., whether the neuron's prediction is slightly off or wildly incorrect) means that it does not scale the gradient updates to reflect the confidence or error in the prediction. This can lead to inefficient learning and the inability to capture the complex patterns in the data that typically emerge due to non-linear relationships.\n",
        "\n",
        "On the other hand, non-linear activation functions allow the gradients to be scaled appropriately, which can lead to more nuanced updates and better learning, especially for complex tasks where modeling non-linear relationships is essential.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bAdvBKAI_cE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Chain Rule Demonstration**\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer:**\n",
        "There seems to be a misunderstanding in the way the chain rule needs to be applied within the symbolic differentiation environment. Let me clarify the steps outside the symbolic computation to guide you through the process:\n",
        "\n",
        "Given the mean squared error loss function:\n",
        "$ L = \\frac{1}{2}(y_{\\text{true}} - y_{\\text{pred}})^2 $\n",
        "\n",
        "And $ y_{\\text{pred}} $ as the output of the sigmoid function:\n",
        "$ y_{\\text{pred}} = \\frac{1}{1 + e^{-z}} $\n",
        "where $ z = wx + b $.\n",
        "\n",
        "Here is how you would apply the chain rule:\n",
        "\n",
        "1. **Find** $ \\frac{\\partial L}{\\partial y_{\\text{pred}}} $:\n",
        "\n",
        "  $ \\frac{\\partial L}{\\partial y_{\\text{pred}}} = -(y_{\\text{true}} - y_{\\text{pred}}) $\n",
        "    \n",
        "  This is because the derivative of $ \\frac{1}{2}x^2 $ with respect to $ x $ is $ x $, and you then multiply by the chain rule for the inside of the squared term which is $ (y_{\\text{true}} - y_{\\text{pred}})$.\n",
        "\n",
        "2. **Find** $ \\frac{\\partial y_{\\text{pred}}}{\\partial z} $:\n",
        "\n",
        "  $ \\frac{\\partial y_{\\text{pred}}}{\\partial z} = y_{\\text{pred}} \\cdot (1 - y_{\\text{pred}}) $\n",
        "  \n",
        "  This comes from the derivative of the sigmoid function which can be expressed as $ \\sigma(z) \\cdot (1 - \\sigma(z)) $.\n",
        "\n",
        "3. **Find** $ \\frac{\\partial z}{\\partial w} $:\n",
        "    \n",
        "  $ \\frac{\\partial z}{\\partial w} = x$\n",
        "  \n",
        "  This is straightforward as $ z = wx + b $, so the derivative of $ z $ with respect to $ w $ is just $ x $, the input.\n",
        "\n",
        "4. **Combine them using the chain rule** to find $ \\frac{\\partial L}{\\partial w} $:\n",
        "  \n",
        "  $ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y_{\\text{pred}}} \\cdot \\frac{\\partial y_{\\text{pred}}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w} $\n",
        "\n",
        "Putting it all together, we have:\n",
        "\n",
        "$ \\frac{\\partial L}{\\partial w} = -(y_{\\text{true}} - y_{\\text{pred}}) \\cdot y_{\\text{pred}} \\cdot (1 - y_{\\text{pred}}) \\cdot x $\n",
        "\n",
        "This expression tells us how to update $ w $ during backpropagation to minimize the loss $ L $.\n",
        "\n",
        "For the exact symbolic computation, we're running into a limitation with the sympy library's ability to handle derivatives with respect to more complex expressions directly. Normally, we would simplify and combine terms within the symbolic computation environment, but in this case, we would need to proceed with the substitutions manually after computing the derivative with respect to the more basic terms.\n",
        "\n",
        "The steps described are the correct application of the chain rule for this context. However, in practice, the derivation is often handled by software libraries that specialize in automatic differentiation, which can handle these chain rule applications internally.\n",
        "\n",
        "---\n",
        "\n",
        "**GPT Prompt:** considering the formula for dL/dw, wont the formula be affected if x is a very vary large valye like 1 million? // (Essentially, I personally spotted an error with the sigmoid function.)\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer:**\n",
        "Yes, that's correct. In the formula for $\\frac{\\partial L}{\\partial w}$, if $x$ (the input to the neuron) is very large, it can significantly affect the gradient during backpropagation.\n",
        "\n",
        "The formula for the derivative of the loss $L$ with respect to the weight $w$ when using mean squared error and a sigmoid activation function is:\n",
        "\n",
        "$ \\frac{\\partial L}{\\partial w} = -(y_{\\text{true}} - y_{\\text{pred}}) \\cdot y_{\\text{pred}} \\cdot (1 - y_{\\text{pred}}) \\cdot x $\n",
        "\n",
        "Here's what happens if $x$ is very large:\n",
        "\n",
        "- The term $x$ in the derivative will amplify any changes in the weight $w$.\n",
        "- A large $x$ value will lead to a large gradient (assuming the other factors like the error $y_{\\text{true}} - y_{\\text{pred}}$ and the derivative of the sigmoid $y_{\\text{pred}} \\cdot (1 - y_{\\text{pred}})$ are not extremely small).\n",
        "- This large gradient can cause the weights to update by a large amount, potentially leading to overshooting the minimum of the loss function during optimization.\n",
        "- It can cause the learning process to be unstable and may result in the model failing to converge or to converge very slowly.\n",
        "\n",
        "To mitigate this, there are a few techniques commonly used:\n",
        "\n",
        "1. **Feature Scaling:** Normalizing or standardizing the input features so that they are of a similar scale. This prevents any feature with a naturally larger scale from disproportionately affecting the gradient.\n",
        "\n",
        "2. **Weight Initialization:** Choosing an appropriate weight initialization strategy can prevent gradients from becoming too large at the start of training.\n",
        "\n",
        "3. **Gradient Clipping:** This technique involves scaling down gradients when they exceed a certain threshold to prevent drastic updates to the weights.\n",
        "\n",
        "4. **Learning Rate Scheduling:** Adjusting the learning rate during training (making it smaller over time) can help manage the size of the weight updates.\n",
        "\n",
        "5. **Use of Regularization:** Regularization methods like L1 or L2 can help control the magnitude of the weights and, by extension, the gradients.\n",
        "\n",
        "By applying these techniques, we can help ensure that large values of $x$ don't cause instability in the neural network training process.\n",
        "\n",
        "---\n",
        "\n",
        "**GPT Prompt:** further, when x = 0, the derivative is also 0? even for dL/dw the y_true is 1, dont this make this method very inaccurate? // but then, if lets say the desired output is 1, if the input is -100, which sigmoid(-100) is approx 0, the derivative would also be close to 0, wont this make sigmoid very ineffective? // (Spotted another error with Sigmoid(x), good way to have a good understanding of things is by graphing and visualising it, then try and plug values into the function left and right.)\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer:**\n",
        "Yes, you are correct on both accounts. Let's address each of your points:\n",
        "\n",
        "1. **Input x = 0:**\n",
        "   If the input \\( x \\) to a neuron is zero, then for the weight update part \\( \\frac{\\partial L}{\\partial w} = -(y_{\\text{true}} - y_{\\text{pred}}) \\cdot y_{\\text{pred}} \\cdot (1 - y_{\\text{pred}}) \\cdot x \\), the term involving \\( x \\) will indeed be zero. This means that regardless of the error \\( (y_{\\text{true}} - y_{\\text{pred}}) \\) and the value of the output from the sigmoid function \\( y_{\\text{pred}} \\), the gradient \\( \\frac{\\partial L}{\\partial w} \\) with respect to the weight \\( w \\) will be zero, so there will be no update to the weight \\( w \\) during that particular iteration of training. This can be problematic if the data is not preprocessed correctly because it means that a particular feature (input) does not contribute to learning.\n",
        "\n",
        "2. **Very Negative Input (e.g., -100):**\n",
        "   For a very negative input to the sigmoid function, like -100, the output of the sigmoid \\( y_{\\text{pred}} \\) will be extremely close to 0. The derivative of the sigmoid function, \\( y_{\\text{pred}}(1 - y_{\\text{pred}}) \\), will also be extremely small since it is maximal at 0.5 and decreases towards 0 as \\( y_{\\text{pred}} \\) approaches 0 (or 1). Consequently, the gradient with respect to the weights, \\( \\frac{\\partial L}{\\partial w} \\), will be small, potentially leading to very small updates to the weights. This is known as the vanishing gradient problem and is a known limitation of the sigmoid activation function.\n",
        "\n",
        "The vanishing gradient problem can make sigmoid activations ineffective for deep networks or for networks where the activation can saturate at either 0 or 1 (such as with very large positive or negative inputs). This is why alternative activation functions like ReLU (Rectified Linear Unit) and its variants, which do not saturate in the positive direction and have a gradient of 1 for positive inputs, have become more popular in deep learning.\n",
        "\n",
        "The image you've shown seems to illustrate the function $ y_{\\text{pred}} $ and its derivative. As you can see, for extreme values of the input, the derivative becomes very small, which is a visualization of the vanishing gradient issue.\n",
        "\n",
        "For networks that use sigmoid activations, careful initialization and normalization of inputs, as well as techniques like gradient clipping, are essential to mitigate these issues.\n"
      ],
      "metadata": {
        "id": "w0MEXniiiR-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT Prompt:**is there any more problems with sigmoid? and at what domains are sigmoid good at\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer:**\n",
        "Yes, besides the vanishing gradient problem, the sigmoid activation function has other limitations when used in neural networks:\n",
        "\n",
        "1. **Saturation and Output Values:**\n",
        "   The sigmoid function squashes its inputs to lie between 0 and 1. While this is useful for probabilities, for example, in the output layer of binary classifiers, it can lead to neurons that are \"killed\" during training. If the neuron's output is consistently near 0 or 1, it may stop learning, as even large changes in the input will not significantly change the output.\n",
        "\n",
        "2. **Not Zero-Centered:**\n",
        "   Because the sigmoid's output is not zero-centered (it ranges from 0 to 1 instead of, say, -1 to 1 like the tanh function), this can lead to gradients being all of the same sign during backpropagation. This can result in zig-zagging dynamics during gradient descent, which can slow down the convergence of the optimization algorithm.\n",
        "\n",
        "3. **Expensive Exponentials:**\n",
        "   The sigmoid function involves an exponential operation, which is computationally more expensive than the alternatives like the ReLU function.\n",
        "\n",
        "4. **Susceptibility to Learning Rate and Initialization:**\n",
        "   Sigmoid units are sensitive to the scale of inputs and the choice of learning rate. Improper initialization or a poorly chosen learning rate can push the majority of the sigmoid units into the saturated regime at the start of training.\n",
        "\n",
        "Despite these limitations, the sigmoid function is still useful and has its advantages in certain domains:\n",
        "\n",
        "1. **Output Layer of Binary Classifiers:**\n",
        "   For binary classification tasks, the sigmoid function is often appropriate for the output layer of a neural network, where it can represent a probability that the input is either in one class or the other.\n",
        "\n",
        "2. **Probabilistic Interpretation:**\n",
        "   In models where a probabilistic interpretation of the outputs is needed, the sigmoid function is suitable because it can be seen as the probability of the input being in a particular class.\n",
        "\n",
        "3. **Small Networks and Shallow Architectures:**\n",
        "   In smaller networks or problems that are linearly separable, the sigmoid function can perform well without running into issues like vanishing gradients.\n",
        "\n",
        "4. **Historical Relevance:**\n",
        "   Sigmoid functions were used in some of the earliest neural networks, and they can still be useful for educational purposes or to reproduce historical models.\n",
        "\n",
        "5. **Gradient-Based Optimization:**\n",
        "   In networks with single-layer or shallow architectures, where vanishing gradients are less of an issue, the smooth gradient of the sigmoid can be an advantage in gradient-based optimization tasks.\n",
        "\n",
        "Today, for hidden layers in deep neural networks, other functions like ReLU and its variants (Leaky ReLU, Parametric ReLU, Exponential Linear Unit, etc.) are typically preferred due to their computational efficiency and their ability to mitigate the vanishing gradient problem. However, sigmoid is still very relevant in specific contexts as mentioned above.\n",
        "\n",
        "---\n",
        "\n",
        "**GPT Prompt:** doesn't this apply to tanh too? its gradient is also the same sign. // \"Because the sigmoid's output is not zero-centered (it ranges from 0 to 1 instead of, say, -1 to 1 like the tanh function), this can lead to gradients being all of the same sign during backpropagation. This can result in zig-zagging dynamics during gradient descent, which can slow down the convergence of the optimization algorithm.\"\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer:**\n",
        "\n",
        "Yes, the tanh function does share some characteristics with the sigmoid function, including the potential for gradients to be of the same sign. However, the issue of gradients all being of the same sign is more specific to the sigmoid function because its output ranges from 0 to 1. Since it never outputs negative values, if the weights in a particular layer of a neural network need to be adjusted downwards, the positive gradients can cause the weights to be adjusted in the wrong direction, leading to the zig-zagging dynamics during optimization.\n",
        "\n",
        "The tanh function, on the other hand, outputs values in the range of -1 to 1 and is zero-centered, which means that its gradients can be positive or negative depending on the input. This helps mitigate the issue of gradients all being of the same sign to some extent. A zero-centered function means that for inputs that are roughly symmetrically distributed around 0, the gradients are also symmetrically distributed around 0, which can lead to more efficient learning updates.\n",
        "\n",
        "However, the tanh function does still suffer from vanishing gradients for inputs with large absolute values. For these inputs, the function becomes flat, and the gradients become very small, which can slow down learning or cause it to stall completely. This is why, despite being zero-centered, tanh is also less commonly used in deep networks compared to functions like ReLU, which do not saturate in the positive direction and therefore maintain larger gradients during learning.\n",
        "\n",
        "---\n",
        "\n",
        "**GPT Prompt:** \"its gradients can be positive or negative depending on the input.\" -- yes but the derivative of arctan x is only positive, how is it any different // (screenshot of desmos graph) // No? as seen in the green graph in my desmos site, the derivative of tanh x is once again positive for both positive and negative input\n",
        "\n",
        "* This prompt and the one above is essentially about GPT's misphrase regarding sigmoid && tanh function's gradient being positive or negative\n",
        "* Further sidenote: I think its referring to the gradient of the MSE / loss function instead.\n",
        "\n",
        "<br>\n",
        "\n",
        "**GPT Answer:**\n",
        "\n",
        "You are correct in pointing out that the derivative of the tanh function is always positive for both positive and negative inputs. **When I mentioned that gradients can be positive or negative, I was referring to the overall gradient of the loss with respect to the weights (including the input term $x$ in the gradient calculation), not just the derivative of the activation function.**\n",
        "\n",
        "For the tanh function, its derivative $ \\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x) $ is indeed always positive. However, the overall gradient of the loss function with respect to the weights in the network takes into account the input $ x $ to the neuron as well as the error signal from the loss function. The sign of the gradient with respect to the weights depends on both the input $ x $ and the error term. If the input $ x $ is negative and the error term is such that the network needs to adjust the weight downwards, the gradient will be negative, leading to an increase in the weight value after the update (because weight updates typically subtract the gradient). Conversely, if $ x $ is positive and the error term indicates the weight needs to decrease, the gradient will be positive, leading to a decrease in the weight after the update.\n",
        "\n",
        "This is different from the sigmoid function where the output and its derivative are always non-negative. The tanh function's output being zero-centered means that for symmetrically distributed inputs, the weight updates will not be biased toward increasing or decreasing the weights but will be determined by the error term.\n",
        "\n",
        "In your Desmos graph, the green graph indeed shows that the derivative of the tanh function is always positive, which is related to the steepness of the tanh curve at any given point $ x $. This positive derivative, while it does mean that the gradient with respect to the activation function is always positive, does not mean that the overall weight updates during backpropagation will always be in the same direction. The input $x$ and the loss gradient $\\frac{\\partial L}{\\partial y_{\\text{pred}}} $ play crucial roles in determining the sign and magnitude of the weight updates.\n",
        "\n",
        "---\n",
        "\n",
        "**Error Term Definition**\n",
        "\n",
        "<br>\n",
        "\n",
        "The \"error term\" typically refers to the difference between the predicted output of the network and the actual target value or ground truth. In the context of backpropagation and weight updates, this error term is used to calculate the gradients of the loss function with respect to the weights.\n",
        "\n",
        "Here’s how it works in a typical supervised learning setting:\n",
        "\n",
        "1. **Prediction:** The neural network makes a prediction $(y_{\\text{pred}}$) based on the current weights and the input data.\n",
        "\n",
        "2. **Error Calculation:** The difference between this prediction and the true target value $(y_{\\text{true}}$) is calculated. This difference is often called the \"error\" or \"loss\". For regression tasks, this is often calculated using the Mean Squared Error (MSE), where the error for each data point is $(y_{\\text{true}} - y_{\\text{pred}})^2$, and for classification tasks, cross-entropy loss might be used instead.\n",
        "\n",
        "3. **Gradient of the Loss Function:** The gradient of the loss function with respect to the weights $(\\frac{\\partial L}{\\partial w}$) is computed using this error. For the MSE, the error term in the gradient would be $(y_{\\text{true}} - y_{\\text{pred}}$). This error term is essential for the backpropagation algorithm because it tells the network the direction in which it needs to adjust the weights to reduce the loss in the next iteration.\n",
        "\n",
        "In the case of using activation functions like sigmoid or tanh in a network layer, the error term is propagated back from the output layer to earlier layers, being adjusted by the derivative of the activation function at each layer. This is due to the chain rule being applied in the computation of the gradients during backpropagation.\n",
        "\n",
        "The overall sign and magnitude of the weight update depend on both the sign and magnitude of this error term and the derivative of the activation function at the value that the neuron outputted before applying the loss function."
      ],
      "metadata": {
        "id": "S1oGyTTxpodg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Ensemble Modelling Definition URL\n",
        "\n",
        "---\n",
        "\n",
        "**GPT Question:**\n",
        "\n",
        "**GPT Answer:**"
      ],
      "metadata": {
        "id": "cX0nxW6C39ea"
      }
    }
  ]
}